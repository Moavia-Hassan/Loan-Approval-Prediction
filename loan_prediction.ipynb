{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style>\n",
    "    .title { color: #2E86C1; font-size: 2.5em; text-align: center; font-weight: bold; }\n",
    "    .subtitle { color: #2874A6; font-size: 1.8em; font-weight: bold; }\n",
    "    .section { color: #1B4F72; font-size: 1.4em; font-weight: bold; }\n",
    "    .highlight { background-color: #D5F5E3; padding: 4px 8px; border-radius: 4px; }\n",
    "</style>\n",
    "\n",
    "<div class=\"title\">ğŸ¦ Loan Approval Prediction Using Logistic Regression</div>\n",
    "\n",
    "<div class=\"subtitle\">ğŸ“Œ Project Overview</div>\n",
    "<p>\n",
    "    Loan approval is a critical process for financial institutions. To streamline decision-making, I am building a \n",
    "    <span class=\"highlight\">logistic regression model</span> that predicts whether a loan application will be approved based on key features such as:\n",
    "</p>\n",
    "\n",
    "<ul>\n",
    "    <li>âœ” <b>Applicant Income</b></li>\n",
    "    <li>âœ” <b>Coapplicant Income</b></li>\n",
    "    <li>âœ” <b>Loan Amount & Term</b></li>\n",
    "    <li>âœ” <b>Credit History</b></li>\n",
    "</ul>\n",
    "\n",
    "<div class=\"subtitle\">ğŸ¯ Objective</div>\n",
    "<p>\n",
    "    The goal is to develop a <b>data-driven model</b> that helps in making accurate loan approval predictions. \n",
    "    This project follows a structured workflow:\n",
    "</p>\n",
    "\n",
    "<ul>\n",
    "    <li>ğŸ”¹ <b>Data Cleaning & Preprocessing</b></li>\n",
    "    <li>ğŸ”¹ <b>Exploratory Data Analysis (EDA)</b></li>\n",
    "    <li>ğŸ”¹ <b>Feature Engineering</b></li>\n",
    "    <li>ğŸ”¹ <b>Model Building & Evaluation</b></li>\n",
    "</ul>\n",
    "\n",
    "<p>\n",
    "    By the end of this project, I aim to achieve a well-tuned logistic regression model that balances <b>accuracy</b> and <b>interpretability</b>.\n",
    "</p>\n",
    "\n",
    "<div class=\"subtitle\">ğŸ“‚ Dataset</div>\n",
    "<p>\n",
    "    The dataset used in this project can be accessed from \n",
    "    <a href=\"https://www.kaggle.com/datasets/ninzaami/loan-predication\" target=\"_blank\"><b>this Kaggle link</b></a>.\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“¥ Loading the Dataset  \n",
    "\n",
    "To begin, I load the **Loan Approval Dataset** into a Pandas DataFrame. The dataset contains **applicant details, income information, loan attributes, and approval status**. Hereâ€™s a glimpse of the data:\n",
    "\n",
    "ğŸ”¹ **Categorical Features**: Gender, Married, Education, Self-Employed, Property Area, Loan Status  \n",
    "ğŸ”¹ **Numerical Features**: Applicant Income, Coapplicant Income, Loan Amount, Loan Term, Credit History  \n",
    "\n",
    "Before proceeding, I'll perform **data cleaning** to handle missing values and inconsistencies. ğŸš€\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Loan_ID Gender Married Dependents     Education Self_Employed  \\\n",
      "0  LP001002   Male      No          0      Graduate            No   \n",
      "1  LP001003   Male     Yes          1      Graduate            No   \n",
      "2  LP001005   Male     Yes          0      Graduate           Yes   \n",
      "3  LP001006   Male     Yes          0  Not Graduate            No   \n",
      "4  LP001008   Male      No          0      Graduate            No   \n",
      "\n",
      "   ApplicantIncome  CoapplicantIncome  LoanAmount  Loan_Amount_Term  \\\n",
      "0             5849                0.0         NaN             360.0   \n",
      "1             4583             1508.0       128.0             360.0   \n",
      "2             3000                0.0        66.0             360.0   \n",
      "3             2583             2358.0       120.0             360.0   \n",
      "4             6000                0.0       141.0             360.0   \n",
      "\n",
      "   Credit_History Property_Area Loan_Status  \n",
      "0             1.0         Urban           Y  \n",
      "1             1.0         Rural           N  \n",
      "2             1.0         Urban           Y  \n",
      "3             1.0         Urban           Y  \n",
      "4             1.0         Urban           Y  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('loan_approval.csv') \n",
    "print(df.head()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ” Data Inspection & Summary\n",
    "\n",
    "Before diving deeper, it's important to examine the dataset's structure and check for any missing or outlier values. I'll first review:\n",
    "\n",
    "1. **Dataset Info** â€“ To check the data types and non-null counts.\n",
    "2. **Missing Values** â€“ To see if there are any missing entries in the columns.\n",
    "3. **Statistical Summary** â€“ To understand the distribution of numerical features.\n",
    "4. **Unique Values in 'Gender' Column** â€“ To check the categories of gender for potential encoding.\n",
    "\n",
    "This step will help me identify issues like missing values, incorrect data types, or outliers, which need to be addressed for further analysis. ğŸš€\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 614 entries, 0 to 613\n",
      "Data columns (total 13 columns):\n",
      " #   Column             Non-Null Count  Dtype  \n",
      "---  ------             --------------  -----  \n",
      " 0   Loan_ID            614 non-null    object \n",
      " 1   Gender             601 non-null    object \n",
      " 2   Married            611 non-null    object \n",
      " 3   Dependents         599 non-null    object \n",
      " 4   Education          614 non-null    object \n",
      " 5   Self_Employed      582 non-null    object \n",
      " 6   ApplicantIncome    614 non-null    int64  \n",
      " 7   CoapplicantIncome  614 non-null    float64\n",
      " 8   LoanAmount         592 non-null    float64\n",
      " 9   Loan_Amount_Term   600 non-null    float64\n",
      " 10  Credit_History     564 non-null    float64\n",
      " 11  Property_Area      614 non-null    object \n",
      " 12  Loan_Status        614 non-null    object \n",
      "dtypes: float64(4), int64(1), object(8)\n",
      "memory usage: 62.5+ KB\n",
      "None\n",
      "Loan_ID               0\n",
      "Gender               13\n",
      "Married               3\n",
      "Dependents           15\n",
      "Education             0\n",
      "Self_Employed        32\n",
      "ApplicantIncome       0\n",
      "CoapplicantIncome     0\n",
      "LoanAmount           22\n",
      "Loan_Amount_Term     14\n",
      "Credit_History       50\n",
      "Property_Area         0\n",
      "Loan_Status           0\n",
      "dtype: int64\n",
      "       ApplicantIncome  CoapplicantIncome  LoanAmount  Loan_Amount_Term  \\\n",
      "count       614.000000         614.000000  592.000000         600.00000   \n",
      "mean       5403.459283        1621.245798  146.412162         342.00000   \n",
      "std        6109.041673        2926.248369   85.587325          65.12041   \n",
      "min         150.000000           0.000000    9.000000          12.00000   \n",
      "25%        2877.500000           0.000000  100.000000         360.00000   \n",
      "50%        3812.500000        1188.500000  128.000000         360.00000   \n",
      "75%        5795.000000        2297.250000  168.000000         360.00000   \n",
      "max       81000.000000       41667.000000  700.000000         480.00000   \n",
      "\n",
      "       Credit_History  \n",
      "count      564.000000  \n",
      "mean         0.842199  \n",
      "std          0.364878  \n",
      "min          0.000000  \n",
      "25%          1.000000  \n",
      "50%          1.000000  \n",
      "75%          1.000000  \n",
      "max          1.000000  \n",
      "['Male' 'Female' nan]\n"
     ]
    }
   ],
   "source": [
    "print(df.info())\n",
    "print(df.isnull().sum())\n",
    "print(df.describe())\n",
    "print(df['Gender'].unique())  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ§¹ Handling Missing Data  \n",
    "\n",
    "The dataset contains several missing values across different columns. To ensure no missing values impact the model's performance, I'll fill these missing values using the most appropriate method:\n",
    "\n",
    "- **Numerical columns like 'LoanAmount'** will be filled with the **median** value to avoid skewing the data.\n",
    "- **Categorical columns like 'Gender', 'Married', 'Dependents', 'Self_Employed', and 'Credit_History'** will be filled with the **mode** (most frequent value) to maintain the category distribution.\n",
    "\n",
    "After filling in the missing values, I'll verify that no more missing data exists in the dataset. This ensures that the model will not encounter any issues when training.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loan_ID               0\n",
      "Gender               13\n",
      "Married               3\n",
      "Dependents           15\n",
      "Education             0\n",
      "Self_Employed        32\n",
      "ApplicantIncome       0\n",
      "CoapplicantIncome     0\n",
      "LoanAmount           22\n",
      "Loan_Amount_Term     14\n",
      "Credit_History       50\n",
      "Property_Area         0\n",
      "Loan_Status           0\n",
      "dtype: int64\n",
      "Loan_ID              0\n",
      "Gender               0\n",
      "Married              0\n",
      "Dependents           0\n",
      "Education            0\n",
      "Self_Employed        0\n",
      "ApplicantIncome      0\n",
      "CoapplicantIncome    0\n",
      "LoanAmount           0\n",
      "Loan_Amount_Term     0\n",
      "Credit_History       0\n",
      "Property_Area        0\n",
      "Loan_Status          0\n",
      "dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MoaviaHassan\\AppData\\Local\\Temp\\ipykernel_3744\\963934493.py:3: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['LoanAmount'].fillna(df['LoanAmount'].median(), inplace=True)\n",
      "C:\\Users\\MoaviaHassan\\AppData\\Local\\Temp\\ipykernel_3744\\963934493.py:4: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['Gender'].fillna(df['Gender'].mode()[0], inplace=True)\n",
      "C:\\Users\\MoaviaHassan\\AppData\\Local\\Temp\\ipykernel_3744\\963934493.py:5: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['Married'].fillna(df['Married'].mode()[0], inplace=True)\n",
      "C:\\Users\\MoaviaHassan\\AppData\\Local\\Temp\\ipykernel_3744\\963934493.py:6: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['Dependents'].fillna(df['Dependents'].mode()[0], inplace=True)\n",
      "C:\\Users\\MoaviaHassan\\AppData\\Local\\Temp\\ipykernel_3744\\963934493.py:7: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['Self_Employed'].fillna(df['Self_Employed'].mode()[0], inplace=True)\n",
      "C:\\Users\\MoaviaHassan\\AppData\\Local\\Temp\\ipykernel_3744\\963934493.py:8: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['Credit_History'].fillna(df['Credit_History'].mode()[0], inplace=True)\n",
      "C:\\Users\\MoaviaHassan\\AppData\\Local\\Temp\\ipykernel_3744\\963934493.py:9: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['Loan_Amount_Term'].fillna(df['Loan_Amount_Term'].mode()[0], inplace=True)\n"
     ]
    }
   ],
   "source": [
    "print(df.isnull().sum())\n",
    "\n",
    "df['LoanAmount'].fillna(df['LoanAmount'].median(), inplace=True)\n",
    "df['Gender'].fillna(df['Gender'].mode()[0], inplace=True)\n",
    "df['Married'].fillna(df['Married'].mode()[0], inplace=True)\n",
    "df['Dependents'].fillna(df['Dependents'].mode()[0], inplace=True)\n",
    "df['Self_Employed'].fillna(df['Self_Employed'].mode()[0], inplace=True)\n",
    "df['Credit_History'].fillna(df['Credit_History'].mode()[0], inplace=True)\n",
    "df['Loan_Amount_Term'].fillna(df['Loan_Amount_Term'].mode()[0], inplace=True)\n",
    "print(df.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸš¨ Handling Outliers in 'LoanAmount'\n",
    "\n",
    "Outliers can significantly impact the performance of machine learning models, especially in regression tasks. Therefore, I will address outliers in the **'LoanAmount'** column using the **Interquartile Range (IQR)** method.\n",
    "\n",
    "The steps involved:\n",
    "1. **Calculate the first (Q1) and third (Q3) quartiles** to determine the IQR.\n",
    "2. **Define the lower and upper bounds** for normal values.\n",
    "3. **Cap values** that fall outside the bounds to the closest limit.\n",
    "\n",
    "This method ensures that extreme outliers are treated while keeping the distribution intact.\n",
    "\n",
    "After applying this, I'll check the distribution of 'LoanAmount' again to confirm the adjustments. ğŸ“Š\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    614.000000\n",
      "mean     137.365635\n",
      "std       55.779749\n",
      "min        9.000000\n",
      "25%      100.250000\n",
      "50%      128.000000\n",
      "75%      164.750000\n",
      "max      261.500000\n",
      "Name: LoanAmount, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "Q1 = df['LoanAmount'].quantile(0.25)\n",
    "Q3 = df['LoanAmount'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "df['LoanAmount'] = df['LoanAmount'].apply(lambda x: upper_bound if x > upper_bound else (lower_bound if x < lower_bound else x))\n",
    "\n",
    "print(df['LoanAmount'].describe())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ”¢ Encoding Categorical Variables\n",
    "\n",
    "To ensure the machine learning model can process categorical features, I will **encode** them into numerical format. This is done using **one-hot encoding**, which creates binary columns for each category. For efficiency, I will **drop the first category** in each column to avoid multicollinearity (the \"dummy variable trap\").\n",
    "\n",
    "Columns to be encoded:\n",
    "- **'Gender'**, **'Married'**, **'Education'**, **'Self_Employed'**, and **'Property_Area'**\n",
    "\n",
    "By applying this transformation, the model will be able to interpret categorical variables as numerical input without any assumptions of ordinal relationships.  \n",
    "\n",
    "After encoding, I will check the updated dataset to confirm the changes. ğŸ“Š\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Loan_ID Dependents  ApplicantIncome  CoapplicantIncome  LoanAmount  \\\n",
      "0  LP001002          0             5849                0.0       128.0   \n",
      "1  LP001003          1             4583             1508.0       128.0   \n",
      "2  LP001005          0             3000                0.0        66.0   \n",
      "3  LP001006          0             2583             2358.0       120.0   \n",
      "4  LP001008          0             6000                0.0       141.0   \n",
      "\n",
      "   Loan_Amount_Term  Credit_History Loan_Status  Gender_Male  Married_Yes  \\\n",
      "0             360.0             1.0           Y         True        False   \n",
      "1             360.0             1.0           N         True         True   \n",
      "2             360.0             1.0           Y         True         True   \n",
      "3             360.0             1.0           Y         True         True   \n",
      "4             360.0             1.0           Y         True        False   \n",
      "\n",
      "   Education_Not Graduate  Self_Employed_Yes  Property_Area_Semiurban  \\\n",
      "0                   False              False                    False   \n",
      "1                   False              False                    False   \n",
      "2                   False               True                    False   \n",
      "3                    True              False                    False   \n",
      "4                   False              False                    False   \n",
      "\n",
      "   Property_Area_Urban  \n",
      "0                 True  \n",
      "1                False  \n",
      "2                 True  \n",
      "3                 True  \n",
      "4                 True  \n"
     ]
    }
   ],
   "source": [
    "df = pd.get_dummies(df, columns=['Gender', 'Married', 'Education', 'Self_Employed', 'Property_Area'], drop_first=True)\n",
    "\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ’¾ Saving the Cleaned Dataset\n",
    "\n",
    "After handling missing values, outliers, and encoding categorical variables, the dataset is now ready for use in training models. To preserve these changes, I will **save the cleaned dataset** to a new CSV file called **'cleaned_loan_prediction.csv'**. This will allow me to easily load the dataset for future analysis or model training.\n",
    "\n",
    "The file will not include the index, keeping the structure neat and consistent with the original data.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('cleaned_loan_prediction.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“Š Exploratory Data Analysis (EDA)\n",
    "\n",
    "Now that the data is cleaned, it's important to **perform some initial exploratory analysis** to understand the dataset better.\n",
    "\n",
    "1. **Descriptive Statistics**: I will start by looking at the summary statistics for numerical columns, such as **ApplicantIncome**, **LoanAmount**, and **Credit_History**. This helps in understanding the central tendency, spread, and any potential anomalies.\n",
    "   \n",
    "2. **Frequency Distribution**: I will also display the frequency distribution for categorical columns, specifically the **'Loan_Status'** (target variable) and **'Credit_History'**, to see how balanced the target is and the distribution of other key features.\n",
    "\n",
    "### Key Observations:\n",
    "- The **Loan_Status** column shows that there are more **'Y'** (approved) loans compared to **'N'** (not approved), which gives an insight into the data's balance.\n",
    "- The **Credit_History** column mostly has values of 1.0 (indicating a positive credit history), with a small number of 0.0 entries (indicating a negative credit history).\n",
    "\n",
    "This analysis gives a good foundation for deciding the next steps in model building. ğŸš€\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       ApplicantIncome  CoapplicantIncome  LoanAmount  Loan_Amount_Term  \\\n",
      "count       614.000000         614.000000  614.000000        614.000000   \n",
      "mean       5403.459283        1621.245798  137.365635        342.410423   \n",
      "std        6109.041673        2926.248369   55.779749         64.428629   \n",
      "min         150.000000           0.000000    9.000000         12.000000   \n",
      "25%        2877.500000           0.000000  100.250000        360.000000   \n",
      "50%        3812.500000        1188.500000  128.000000        360.000000   \n",
      "75%        5795.000000        2297.250000  164.750000        360.000000   \n",
      "max       81000.000000       41667.000000  261.500000        480.000000   \n",
      "\n",
      "       Credit_History  \n",
      "count      614.000000  \n",
      "mean         0.855049  \n",
      "std          0.352339  \n",
      "min          0.000000  \n",
      "25%          1.000000  \n",
      "50%          1.000000  \n",
      "75%          1.000000  \n",
      "max          1.000000  \n",
      "Loan_Status\n",
      "Y    422\n",
      "N    192\n",
      "Name: count, dtype: int64\n",
      "Credit_History\n",
      "1.0    525\n",
      "0.0     89\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('cleaned_loan_prediction.csv')\n",
    "\n",
    "print(df.describe())\n",
    "\n",
    "print(df['Loan_Status'].value_counts())  # Target variable\n",
    "print(df['Credit_History'].value_counts())  # Example for another categorical column\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“Š Statistical Testing: Chi-square Test\n",
    "\n",
    "Next, I will perform a **Chi-square test** to analyze the relationship between **Credit_History** (a categorical feature) and **Loan_Status** (the target variable). The goal is to assess whether there is a significant association between the applicant's credit history and the loan approval status.\n",
    "\n",
    "### **Chi-square Test Results:**\n",
    "- **Chi2 Statistic**: 176.11 â€” This is the test statistic calculated from the observed and expected frequencies.\n",
    "- **P-value**: 3.42e-40 â€” A very low p-value suggests strong evidence against the null hypothesis, meaning there is a statistically significant relationship between **Credit_History** and **Loan_Status**.\n",
    "- **Degrees of Freedom**: 1 â€” The degrees of freedom for this test, calculated as the number of categories in **Credit_History** minus one, multiplied by the number of categories in **Loan_Status** minus one.\n",
    "- **Expected Frequencies**: A table showing the expected counts for each combination of categories.\n",
    "\n",
    "Since the p-value is very small (less than 0.05), I can confidently reject the null hypothesis, concluding that **Credit_History** has a significant influence on **Loan_Status**.\n",
    "\n",
    "This statistical test helps provide insight into which features are worth considering in the model. ğŸš€\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chi-square Test Results:\n",
      "Chi2 Statistic: 176.1145746235241\n",
      "P-value: 3.4183499979091188e-40\n",
      "Degrees of Freedom: 1\n",
      "Expected Frequencies Table:\n",
      "[[ 27.83061889  61.16938111]\n",
      " [164.16938111 360.83061889]]\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "contingency_table = pd.crosstab(df['Credit_History'], df['Loan_Status'])\n",
    "\n",
    "chi2, p_value, dof, expected = chi2_contingency(contingency_table)\n",
    "\n",
    "print(\"Chi-square Test Results:\")\n",
    "print(f\"Chi2 Statistic: {chi2}\")\n",
    "print(f\"P-value: {p_value}\")\n",
    "print(f\"Degrees of Freedom: {dof}\")\n",
    "print(\"Expected Frequencies Table:\")\n",
    "print(expected)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ§‘â€ğŸ’¼ T-test for Independent Samples: ApplicantIncome\n",
    "\n",
    "Now, I will conduct a **t-test** to compare the **ApplicantIncome** between the approved and rejected loan groups. The t-test helps assess whether there is a significant difference in the mean income between the two groups.\n",
    "\n",
    "### **T-test Results for ApplicantIncome:**\n",
    "- **t-statistic**: -0.1165 â€” The t-statistic is close to zero, indicating a minimal difference between the two groups.\n",
    "- **p-value**: 0.9073 â€” Since the p-value is much greater than 0.05, I fail to reject the null hypothesis. This suggests that there is **no significant difference** in the average income between approved and rejected loan applicants.\n",
    "\n",
    "### **Conclusion:**\n",
    "The lack of a significant result implies that **ApplicantIncome** alone may not be a strong predictor for loan approval, which might suggest considering other features or transforming this feature for better model performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t-test Results for ApplicantIncome:\n",
      "t-statistic: -0.11650844828724542\n",
      "p-value: 0.907287812130518\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import ttest_ind\n",
    "\n",
    "income_approved = df[df['Loan_Status'] == 'Y']['ApplicantIncome']\n",
    "income_rejected = df[df['Loan_Status'] == 'N']['ApplicantIncome']\n",
    "\n",
    "t_stat, p_value = ttest_ind(income_approved, income_rejected)\n",
    "\n",
    "print(\"t-test Results for ApplicantIncome:\")\n",
    "print(f\"t-statistic: {t_stat}\")\n",
    "print(f\"p-value: {p_value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variance Inflation Factor (VIF) Calculation\n",
    "\n",
    "In this step, I calculate the Variance Inflation Factor (VIF) for each independent variable. VIF measures the extent to which the variance of an estimated regression coefficient increases due to collinearity with other predictors. A higher VIF indicates high multicollinearity.\n",
    "\n",
    "Here, I focus on the following variables: \n",
    "- `ApplicantIncome`\n",
    "- `CoapplicantIncome`\n",
    "- `LoanAmount`\n",
    "- `Loan_Amount_Term`\n",
    "- `Credit_History`\n",
    "\n",
    "Typically, a VIF above 5-10 suggests problematic multicollinearity, which might affect the modelâ€™s accuracy. If necessary, I will consider transforming or removing variables with high VIF values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VIF Results:\n",
      "            Variable       VIF\n",
      "0    ApplicantIncome  2.321698\n",
      "1  CoapplicantIncome  1.456320\n",
      "2         LoanAmount  9.008554\n",
      "3   Loan_Amount_Term  9.639508\n",
      "4     Credit_History  5.871926\n"
     ]
    }
   ],
   "source": [
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "independent_vars = df[['ApplicantIncome', 'CoapplicantIncome', 'LoanAmount', 'Loan_Amount_Term', 'Credit_History']]\n",
    "\n",
    "vif_data = pd.DataFrame()\n",
    "vif_data['Variable'] = independent_vars.columns\n",
    "vif_data['VIF'] = [variance_inflation_factor(independent_vars.values, i) for i in range(independent_vars.shape[1])]\n",
    "\n",
    "print(\"VIF Results:\")\n",
    "print(vif_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling Multicollinearity by Removing Highly Correlated Variables\n",
    "\n",
    "After analyzing the Variance Inflation Factor (VIF) values, it was observed that `LoanAmount` and `Loan_Amount_Term` have high VIF, indicating multicollinearity. To address this, we will remove the `Loan_Amount_Term` variable and recalculate the VIF for the remaining features. By doing this, we reduce multicollinearity and make the model more stable.\n",
    "\n",
    "#### Code to Remove `Loan_Amount_Term` and Recalculate VIF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated VIF Results after removing Loan_Amount_Term:\n",
      "            Variable       VIF\n",
      "0    ApplicantIncome  2.303778\n",
      "1  CoapplicantIncome  1.450722\n",
      "2         LoanAmount  5.975299\n",
      "3     Credit_History  3.791828\n"
     ]
    }
   ],
   "source": [
    "df = df.drop(columns=['Loan_Amount_Term'])\n",
    "\n",
    "independent_vars = df[['ApplicantIncome', 'CoapplicantIncome', 'LoanAmount', 'Credit_History']]\n",
    "vif_data = pd.DataFrame()\n",
    "vif_data['Variable'] = independent_vars.columns\n",
    "vif_data['VIF'] = [variance_inflation_factor(independent_vars.values, i) for i in range(independent_vars.shape[1])]\n",
    "\n",
    "print(\"Updated VIF Results after removing Loan_Amount_Term:\")\n",
    "print(vif_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding \"Monthly Loan Payment\" Feature\n",
    "\n",
    "We will add a new feature **\"Monthly Loan Payment\"** based on a standard loan payment formula. The formula for monthly payments is:\n",
    "\n",
    "\\[\n",
    "M = \\frac{P \\times r \\times (1 + r)^n}{(1 + r)^n - 1}\n",
    "\\]\n",
    "\n",
    "Where:\n",
    "- **M** is the monthly payment.\n",
    "- **P** is the loan amount.\n",
    "- **r** is the monthly interest rate (annual rate divided by 12).\n",
    "- **n** is the number of payments (loan term in months).\n",
    "\n",
    "We will also handle outliers in the newly created feature to ensure its validity.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   LoanAmount  Monthly_Loan_Payment\n",
      "0       128.0              1.123292\n",
      "1       128.0              1.123292\n",
      "2        66.0              0.579197\n",
      "3       120.0              1.053086\n",
      "4       141.0              1.237376\n",
      "Cleaned Monthly Loan Payment Feature:\n",
      "0    1.123292\n",
      "1    1.123292\n",
      "2    0.579197\n",
      "3    1.053086\n",
      "4    1.237376\n",
      "Name: Monthly_Loan_Payment, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "annual_interest_rate = 0.10\n",
    "monthly_interest_rate = annual_interest_rate / 12\n",
    "loan_term = 360\n",
    "\n",
    "df['Monthly_Loan_Payment'] = df['LoanAmount'] * monthly_interest_rate * (1 + monthly_interest_rate) ** loan_term / ((1 + monthly_interest_rate) ** loan_term - 1)\n",
    "\n",
    "print(df[['LoanAmount', 'Monthly_Loan_Payment']].head())\n",
    "\n",
    "Q1 = df['Monthly_Loan_Payment'].quantile(0.25)\n",
    "Q3 = df['Monthly_Loan_Payment'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "df['Monthly_Loan_Payment'] = df['Monthly_Loan_Payment'].apply(lambda x: upper_bound if x > upper_bound else (lower_bound if x < lower_bound else x))\n",
    "\n",
    "print(\"Cleaned Monthly Loan Payment Feature:\")\n",
    "print(df['Monthly_Loan_Payment'].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding the \"Monthly Loan Payment\" feature and Handling Outliers\n",
    "\n",
    "In this step, we will create a new feature \"Monthly Loan Payment\" based on the formula for monthly payments of a loan. We'll assume a fixed interest rate and a standard loan term of 30 years (360 months).\n",
    "\n",
    "Then, we'll handle outliers for the \"Monthly Loan Payment\" feature by applying the Interquartile Range (IQR) method to cap the values within the upper and lower bounds.\n",
    "\n",
    "The formula for the monthly payment is:\n",
    "\n",
    "\\[\n",
    "M = \\frac{P \\cdot r \\cdot (1 + r)^n}{(1 + r)^n - 1}\n",
    "\\]\n",
    "\n",
    "Where:\n",
    "- \\( P \\) is the loan amount\n",
    "- \\( r \\) is the monthly interest rate (annual rate / 12)\n",
    "- \\( n \\) is the loan term in months (360 months for a 30-year term)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   LoanAmount  Monthly_Loan_Payment\n",
      "0       128.0              1.123292\n",
      "1       128.0              1.123292\n",
      "2        66.0              0.579197\n",
      "3       120.0              1.053086\n",
      "4       141.0              1.237376\n"
     ]
    }
   ],
   "source": [
    "annual_interest_rate = 0.1\n",
    "r = annual_interest_rate / 12\n",
    "n = 360\n",
    "\n",
    "df['Monthly_Loan_Payment'] = (df['LoanAmount'] * r * (1 + r)**n) / ((1 + r)**n - 1)\n",
    "\n",
    "Q1 = df['Monthly_Loan_Payment'].quantile(0.25)\n",
    "Q3 = df['Monthly_Loan_Payment'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "df['Monthly_Loan_Payment'] = df['Monthly_Loan_Payment'].apply(lambda x: upper_bound if x > upper_bound else (lower_bound if x < lower_bound else x))\n",
    "\n",
    "print(df[['LoanAmount', 'Monthly_Loan_Payment']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    1\n",
      "1    0\n",
      "2    1\n",
      "3    1\n",
      "4    1\n",
      "Name: Loan_Status, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df['Loan_Status'] = df['Loan_Status'].map({'Y': 1, 'N': 0})\n",
    "\n",
    "print(df['Loan_Status'].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting the Data into Training and Testing Sets\n",
    "\n",
    "Now that we have preprocessed and encoded the data, the next step is to split it into training and testing sets. This will allow us to train our machine learning model on one subset of the data and evaluate its performance on another.\n",
    "\n",
    "We'll use the `train_test_split` function from `sklearn.model_selection` to split the data, with 80% for training and 20% for testing.\n",
    "\n",
    "```python\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df.drop(columns=['Loan_Status'])\n",
    "y = df['Loan_Status']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"Training set size: {X_train.shape[0]}\")\n",
    "print(f\"Testing set size: {X_test.shape[0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First few rows of the dataframe:\n",
      "  Dependents  ApplicantIncome  CoapplicantIncome  LoanAmount  Loan_Status  \\\n",
      "0          0             5849                0.0       128.0            1   \n",
      "1          1             4583             1508.0       128.0            0   \n",
      "2          0             3000                0.0        66.0            1   \n",
      "3          0             2583             2358.0       120.0            1   \n",
      "4          0             6000                0.0       141.0            1   \n",
      "\n",
      "   Gender_Male  Married_Yes  Education_Not Graduate  Self_Employed_Yes  \\\n",
      "0         True        False                   False              False   \n",
      "1         True         True                   False              False   \n",
      "2         True         True                   False               True   \n",
      "3         True         True                    True              False   \n",
      "4         True        False                   False              False   \n",
      "\n",
      "   Property_Area_Semiurban  Property_Area_Urban  Monthly_Loan_Payment  \\\n",
      "0                    False                 True              1.123292   \n",
      "1                    False                False              1.123292   \n",
      "2                    False                 True              0.579197   \n",
      "3                    False                 True              1.053086   \n",
      "4                    False                 True              1.237376   \n",
      "\n",
      "   Credit_History_1.0  \n",
      "0                True  \n",
      "1                True  \n",
      "2                True  \n",
      "3                True  \n",
      "4                True  \n",
      "\n",
      "Missing values in each column:\n",
      "Dependents                 0\n",
      "ApplicantIncome            0\n",
      "CoapplicantIncome          0\n",
      "LoanAmount                 0\n",
      "Loan_Status                0\n",
      "Gender_Male                0\n",
      "Married_Yes                0\n",
      "Education_Not Graduate     0\n",
      "Self_Employed_Yes          0\n",
      "Property_Area_Semiurban    0\n",
      "Property_Area_Urban        0\n",
      "Monthly_Loan_Payment       0\n",
      "Credit_History_1.0         0\n",
      "dtype: int64\n",
      "\n",
      "Data types of each column:\n",
      "Dependents                  object\n",
      "ApplicantIncome              int64\n",
      "CoapplicantIncome          float64\n",
      "LoanAmount                 float64\n",
      "Loan_Status                  int64\n",
      "Gender_Male                   bool\n",
      "Married_Yes                   bool\n",
      "Education_Not Graduate        bool\n",
      "Self_Employed_Yes             bool\n",
      "Property_Area_Semiurban       bool\n",
      "Property_Area_Urban           bool\n",
      "Monthly_Loan_Payment       float64\n",
      "Credit_History_1.0            bool\n",
      "dtype: object\n",
      "\n",
      "Unique values in each column (for non-numeric checks):\n",
      "Dependents: ['0' '1' '2' '3+']\n"
     ]
    }
   ],
   "source": [
    "# Display the first few rows of the dataframe to inspect the data\n",
    "print(\"First few rows of the dataframe:\")\n",
    "print(df.head())\n",
    "\n",
    "# Check for missing values in each column\n",
    "print(\"\\nMissing values in each column:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Check the data types of each column\n",
    "print(\"\\nData types of each column:\")\n",
    "print(df.dtypes)\n",
    "\n",
    "# Check for any non-numeric values in the dataset\n",
    "print(\"\\nUnique values in each column (for non-numeric checks):\")\n",
    "for column in df.select_dtypes(include=['object']).columns:\n",
    "    print(f\"{column}: {df[column].unique()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Unique values in 'Dependents' column after replacement:\n",
      "[0 1 2 3]\n"
     ]
    }
   ],
   "source": [
    "# Replace '3+' with '3' in the 'Dependents' column\n",
    "df['Dependents'] = df['Dependents'].replace('3+', '3')\n",
    "\n",
    "# Convert 'Dependents' column to numeric type\n",
    "df['Dependents'] = pd.to_numeric(df['Dependents'])\n",
    "\n",
    "# Check the unique values again to ensure everything is now numeric\n",
    "print(\"\\nUnique values in 'Dependents' column after replacement:\")\n",
    "print(df['Dependents'].unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data types after handling 'Dependents' column:\n",
      "Dependents                   int64\n",
      "ApplicantIncome              int64\n",
      "CoapplicantIncome          float64\n",
      "LoanAmount                 float64\n",
      "Loan_Status                  int64\n",
      "Gender_Male                   bool\n",
      "Married_Yes                   bool\n",
      "Education_Not Graduate        bool\n",
      "Self_Employed_Yes             bool\n",
      "Property_Area_Semiurban       bool\n",
      "Property_Area_Urban           bool\n",
      "Monthly_Loan_Payment       float64\n",
      "Credit_History_1.0            bool\n",
      "dtype: object\n",
      "\n",
      "Columns after one-hot encoding:\n",
      "Index(['Dependents', 'ApplicantIncome', 'CoapplicantIncome', 'LoanAmount',\n",
      "       'Loan_Status', 'Gender_Male', 'Married_Yes', 'Education_Not Graduate',\n",
      "       'Self_Employed_Yes', 'Property_Area_Semiurban', 'Property_Area_Urban',\n",
      "       'Monthly_Loan_Payment', 'Credit_History_1.0'],\n",
      "      dtype='object')\n",
      "\n",
      "Shapes of train and test sets:\n",
      "X_train shape: (491, 12)\n",
      "X_test shape: (123, 12)\n"
     ]
    }
   ],
   "source": [
    "# Check for any remaining categorical variables\n",
    "print(\"\\nData types after handling 'Dependents' column:\")\n",
    "print(df.dtypes)\n",
    "\n",
    "# Convert categorical columns (binary variables) to numeric using one-hot encoding\n",
    "df = pd.get_dummies(df, drop_first=True)\n",
    "\n",
    "# Verify the transformation\n",
    "print(\"\\nColumns after one-hot encoding:\")\n",
    "print(df.columns)\n",
    "\n",
    "# Split the data into features (X) and target variable (y)\n",
    "X = df.drop(columns=['Loan_Status'])\n",
    "y = df['Loan_Status']\n",
    "\n",
    "# Split the data into training and test sets (80-20 split)\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Print the shapes of the train and test sets to confirm the split\n",
    "print(\"\\nShapes of train and test sets:\")\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Performance:\n",
      "Accuracy: 0.7886\n",
      "\n",
      "Confusion Matrix:\n",
      "[[18 25]\n",
      " [ 1 79]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.42      0.58        43\n",
      "           1       0.76      0.99      0.86        80\n",
      "\n",
      "    accuracy                           0.79       123\n",
      "   macro avg       0.85      0.70      0.72       123\n",
      "weighted avg       0.83      0.79      0.76       123\n",
      "\n",
      "\n",
      "Model Coefficients:\n",
      "[[ 1.30809473e-01 -9.84922412e-06 -4.68698268e-05 -2.65036908e-03\n",
      "  -6.08030330e-02  5.66063997e-01 -3.48200090e-01  1.17373462e-01\n",
      "   7.87616787e-01  1.05070529e-01 -2.32588850e-05  3.26024496e+00]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\MoaviaHassan\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "# Initialize the logistic regression model\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "\n",
    "# Train the model on the training data\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model performance\n",
    "print(\"\\nModel Performance:\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# If needed, you can also print the coefficients of the logistic regression model\n",
    "print(\"\\nModel Coefficients:\")\n",
    "print(model.coef_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Previous code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated VIF Results:\n",
      "               Variable       VIF\n",
      "0       ApplicantIncome  1.712168\n",
      "1     CoapplicantIncome  1.305973\n",
      "2  Monthly_Loan_Payment  1.916867\n",
      "3        Credit_History  2.219233\n"
     ]
    }
   ],
   "source": [
    "# Select independent variables for VIF calculation\n",
    "independent_vars = df[['ApplicantIncome', 'CoapplicantIncome', 'Monthly_Loan_Payment', 'Credit_History']]\n",
    "\n",
    "# Calculate VIF for each variable\n",
    "vif_data = pd.DataFrame()\n",
    "vif_data['Variable'] = independent_vars.columns\n",
    "vif_data['VIF'] = [variance_inflation_factor(independent_vars.values, i) for i in range(independent_vars.shape[1])]\n",
    "\n",
    "print(\"Updated VIF Results:\")\n",
    "print(vif_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Outlier Detection Using Cookâ€™s Distance\n",
    "Cookâ€™s Distance measures the influence of each data point on the regression model. Points with a Cookâ€™s Distance greater than a threshold (typically 4/n, where n is the number of observations) are considered influential outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.476154\n",
      "         Iterations 6\n",
      "Number of outliers: 23\n",
      "Outlier indices: [9, 122, 155, 171, 183, 201, 262, 267, 284, 308, 326, 333, 369, 402, 417, 453, 487, 497, 527, 546, 581, 585, 600]\n",
      "      Loan_ID Dependents  ApplicantIncome  CoapplicantIncome  Credit_History  \\\n",
      "9    LP001020          1            12841            10968.0             1.0   \n",
      "122  LP001431          0             2137             8980.0             0.0   \n",
      "155  LP001536         3+            39999                0.0             0.0   \n",
      "171  LP001585         3+            51763                0.0             1.0   \n",
      "183  LP001637          1            33846                0.0             1.0   \n",
      "201  LP001677          2             4923                0.0             0.0   \n",
      "262  LP001870          1             3481                0.0             1.0   \n",
      "267  LP001882         3+             4333             1811.0             0.0   \n",
      "284  LP001922          0            20667                0.0             1.0   \n",
      "308  LP001996          0            20233                0.0             1.0   \n",
      "326  LP002068          0             4917                0.0             0.0   \n",
      "333  LP002101          0            63337                0.0             1.0   \n",
      "369  LP002191          0            19730             5266.0             1.0   \n",
      "402  LP002297          0             2500            20000.0             1.0   \n",
      "417  LP002342          2             1600            20000.0             1.0   \n",
      "453  LP002449          0             2483             2466.0             0.0   \n",
      "487  LP002547          1            18333                0.0             1.0   \n",
      "497  LP002588          0             4625             2857.0             1.0   \n",
      "527  LP002706          1             5285             1430.0             0.0   \n",
      "546  LP002768          0             3358                0.0             1.0   \n",
      "581  LP002893          0             1836            33837.0             1.0   \n",
      "585  LP002912          1             4283             3000.0             1.0   \n",
      "600  LP002949         3+              416            41667.0             1.0   \n",
      "\n",
      "     Loan_Status  Gender_Male  Married_Yes  Education_Not Graduate  \\\n",
      "9              0         True         True                   False   \n",
      "122            1        False        False                   False   \n",
      "155            1         True         True                   False   \n",
      "171            1         True         True                   False   \n",
      "183            0         True         True                   False   \n",
      "201            1         True        False                   False   \n",
      "262            0        False        False                   False   \n",
      "267            1         True         True                   False   \n",
      "284            0         True         True                   False   \n",
      "308            0         True        False                   False   \n",
      "326            1         True        False                   False   \n",
      "333            1         True         True                   False   \n",
      "369            0         True         True                   False   \n",
      "402            1         True        False                   False   \n",
      "417            0         True         True                   False   \n",
      "453            1         True         True                   False   \n",
      "487            0         True         True                   False   \n",
      "497            1         True         True                   False   \n",
      "527            1         True         True                    True   \n",
      "546            0         True        False                    True   \n",
      "581            0         True        False                   False   \n",
      "585            0         True         True                   False   \n",
      "600            0        False        False                   False   \n",
      "\n",
      "     Self_Employed_Yes  Property_Area_Semiurban  Property_Area_Urban  \\\n",
      "9                False                     True                False   \n",
      "122              False                     True                False   \n",
      "155              False                     True                False   \n",
      "171              False                    False                 True   \n",
      "183              False                     True                False   \n",
      "201              False                     True                False   \n",
      "262              False                     True                False   \n",
      "267              False                    False                 True   \n",
      "284              False                    False                False   \n",
      "308              False                    False                False   \n",
      "326              False                    False                False   \n",
      "333              False                    False                 True   \n",
      "369              False                    False                False   \n",
      "402              False                     True                False   \n",
      "417               True                    False                 True   \n",
      "453              False                    False                False   \n",
      "487              False                    False                 True   \n",
      "497              False                    False                 True   \n",
      "527              False                     True                False   \n",
      "546              False                     True                False   \n",
      "581              False                    False                 True   \n",
      "585              False                    False                False   \n",
      "600              False                    False                 True   \n",
      "\n",
      "     Monthly_Loan_Payment  Cooks_Distance  \n",
      "9                0.726389        0.012309  \n",
      "122              0.380556        0.038779  \n",
      "155              1.452778        0.101362  \n",
      "171              0.871667        0.006922  \n",
      "183              0.722222        0.039305  \n",
      "201              0.461111        0.027127  \n",
      "262              4.305556        0.076024  \n",
      "267              0.444444        0.027188  \n",
      "284              0.355556        0.012415  \n",
      "308              0.726389        0.011250  \n",
      "326              0.361111        0.027033  \n",
      "333              1.452778        0.013715  \n",
      "369              0.726389        0.011662  \n",
      "402              0.286111        0.012502  \n",
      "417              0.663889        0.027004  \n",
      "453              0.500000        0.028035  \n",
      "487              0.726389        0.008957  \n",
      "497              9.250000        1.037625  \n",
      "527              0.447222        0.026984  \n",
      "546              2.222222        0.016972  \n",
      "581              0.250000        0.066146  \n",
      "585              2.047619        0.011887  \n",
      "600              1.452778        0.062891  \n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "\n",
    "# Prepare the data for logistic regression\n",
    "# Encode the target variable (Loan_Status) as binary (1 for 'Y', 0 for 'N')\n",
    "df['Loan_Status'] = df['Loan_Status'].map({'Y': 1, 'N': 0})\n",
    "\n",
    "# Define independent variables (features) and dependent variable (target)\n",
    "X = df[['ApplicantIncome', 'CoapplicantIncome', 'Monthly_Loan_Payment', 'Credit_History']]\n",
    "X = sm.add_constant(X)  # Add a constant for the intercept term\n",
    "y = df['Loan_Status']\n",
    "\n",
    "# Fit the logistic regression model\n",
    "model = sm.Logit(y, X)\n",
    "results = model.fit()\n",
    "\n",
    "# Calculate Cook's Distance\n",
    "influence = results.get_influence()\n",
    "cooks_distance = influence.cooks_distance[0]\n",
    "\n",
    "# Identify outliers (points with Cook's Distance > 4/n)\n",
    "n = len(df)\n",
    "outlier_threshold = 4 / n\n",
    "outliers = cooks_distance > outlier_threshold\n",
    "\n",
    "# Print the number of outliers and their indices\n",
    "print(f\"Number of outliers: {outliers.sum()}\")\n",
    "print(f\"Outlier indices: {df.index[outliers].tolist()}\")\n",
    "\n",
    "# Add Cook's Distance to the dataframe for visualization\n",
    "df['Cooks_Distance'] = cooks_distance\n",
    "\n",
    "# Display rows with outliers\n",
    "print(df[outliers])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Remove Extreme Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows after removal: 613\n"
     ]
    }
   ],
   "source": [
    "# Remove the row with the highest Cook's Distance\n",
    "df_cleaned = df.drop(index=497)\n",
    "\n",
    "# Verify the removal\n",
    "print(f\"Number of rows after removal: {len(df_cleaned)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.474795\n",
      "         Iterations 6\n",
      "Number of outliers after removal: 24\n",
      "Outlier indices after removal: [9, 68, 122, 155, 183, 201, 242, 262, 267, 284, 308, 326, 333, 369, 402, 409, 417, 453, 487, 527, 546, 581, 585, 600]\n"
     ]
    }
   ],
   "source": [
    "# Prepare the cleaned data for logistic regression\n",
    "X_cleaned = df_cleaned[['ApplicantIncome', 'CoapplicantIncome', 'Monthly_Loan_Payment', 'Credit_History']]\n",
    "X_cleaned = sm.add_constant(X_cleaned)  # Add a constant for the intercept term\n",
    "y_cleaned = df_cleaned['Loan_Status']\n",
    "\n",
    "# Fit the logistic regression model on cleaned data\n",
    "model_cleaned = sm.Logit(y_cleaned, X_cleaned)\n",
    "results_cleaned = model_cleaned.fit()\n",
    "\n",
    "# Calculate Cook's Distance for the cleaned data\n",
    "influence_cleaned = results_cleaned.get_influence()\n",
    "cooks_distance_cleaned = influence_cleaned.cooks_distance[0]\n",
    "\n",
    "# Identify outliers in the cleaned data\n",
    "outlier_threshold_cleaned = 4 / len(df_cleaned)\n",
    "outliers_cleaned = cooks_distance_cleaned > outlier_threshold_cleaned\n",
    "\n",
    "# Print the number of outliers and their indices\n",
    "print(f\"Number of outliers after removal: {outliers_cleaned.sum()}\")\n",
    "print(f\"Outlier indices after removal: {df_cleaned.index[outliers_cleaned].tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Model Building Process**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set shape: (490, 4)\n",
      "Testing set shape: (123, 4)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Define independent variables (features) and dependent variable (target)\n",
    "X = df_cleaned[['ApplicantIncome', 'CoapplicantIncome', 'Monthly_Loan_Payment', 'Credit_History']]\n",
    "y = df_cleaned['Loan_Status']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Check the shape of the splits\n",
    "print(f\"Training set shape: {X_train.shape}\")\n",
    "print(f\"Testing set shape: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Coefficients:\n",
      "[[ 1.27943698e-05 -4.52776624e-05 -5.56031282e-01  3.48525970e+00]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Initialize the logistic regression model\n",
    "model = LogisticRegression()\n",
    "\n",
    "# Train the model on the training data\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Print the model coefficients\n",
    "print(\"Model Coefficients:\")\n",
    "print(model.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7805\n",
      "Precision: 0.7714\n",
      "Recall: 0.9643\n",
      "F1-Score: 0.8571\n",
      "ROC-AUC Score: 0.6745\n",
      "Confusion Matrix:\n",
      "[[15 24]\n",
      " [ 3 81]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
    "\n",
    "# Make predictions on the testing data\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "roc_auc = roc_auc_score(y_test, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1-Score: {f1:.4f}\")\n",
    "print(f\"ROC-AUC Score: {roc_auc:.4f}\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Loan_ID Dependents  ApplicantIncome  CoapplicantIncome  Credit_History  \\\n",
      "0  LP001002          0         0.072724          -0.553427             1.0   \n",
      "1  LP001003          1        -0.134512          -0.038017             1.0   \n",
      "2  LP001005          0        -0.393640          -0.553427             1.0   \n",
      "3  LP001006          0        -0.461900           0.252500             1.0   \n",
      "4  LP001008          0         0.097442          -0.553427             1.0   \n",
      "\n",
      "   Loan_Status  Gender_Male  Married_Yes  Education_Not Graduate  \\\n",
      "0            1         True        False                   False   \n",
      "1            0         True         True                   False   \n",
      "2            1         True         True                   False   \n",
      "3            1         True         True                    True   \n",
      "4            1         True        False                   False   \n",
      "\n",
      "   Self_Employed_Yes  Property_Area_Semiurban  Property_Area_Urban  \\\n",
      "0              False                    False                 True   \n",
      "1              False                    False                False   \n",
      "2               True                    False                 True   \n",
      "3              False                    False                 True   \n",
      "4              False                    False                 True   \n",
      "\n",
      "   Monthly_Loan_Payment  Cooks_Distance  Total_Income  \n",
      "0             -0.258789        0.000112     -0.181921  \n",
      "1             -0.258789        0.001549     -0.144451  \n",
      "2             -0.834874        0.000137     -0.623035  \n",
      "3             -0.333122        0.000133     -0.322508  \n",
      "4             -0.137996        0.000112     -0.158541  \n"
     ]
    }
   ],
   "source": [
    "# Create a new feature: Total Income\n",
    "df_cleaned['Total_Income'] = df_cleaned['ApplicantIncome'] + df_cleaned['CoapplicantIncome']\n",
    "\n",
    "# Normalize numerical features\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Define numerical features to normalize\n",
    "numerical_features = ['ApplicantIncome', 'CoapplicantIncome', 'Monthly_Loan_Payment', 'Total_Income']\n",
    "\n",
    "# Initialize the scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Normalize the numerical features\n",
    "df_cleaned[numerical_features] = scaler.fit_transform(df_cleaned[numerical_features])\n",
    "\n",
    "# Verify the updated dataset\n",
    "print(df_cleaned.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7805\n",
      "Precision: 0.7714\n",
      "Recall: 0.9643\n",
      "F1-Score: 0.8571\n",
      "ROC-AUC Score: 0.6745\n",
      "Confusion Matrix:\n",
      "[[15 24]\n",
      " [ 3 81]]\n"
     ]
    }
   ],
   "source": [
    "# Define independent variables (features) and dependent variable (target)\n",
    "X = df_cleaned[['ApplicantIncome', 'CoapplicantIncome', 'Monthly_Loan_Payment', 'Credit_History', 'Total_Income']]\n",
    "y = df_cleaned['Loan_Status']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize and train the logistic regression model\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the testing data\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "roc_auc = roc_auc_score(y_test, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1-Score: {f1:.4f}\")\n",
    "print(f\"ROC-AUC Score: {roc_auc:.4f}\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style>\n",
    "    .title { color: #2E86C1; font-size: 2.5em; text-align: center; font-weight: bold; }\n",
    "    .subtitle { color: #2874A6; font-size: 1.8em; font-weight: bold; }\n",
    "    .section { color: #1B4F72; font-size: 1.4em; font-weight: bold; }\n",
    "    .highlight { background-color: #D5F5E3; padding: 4px 8px; border-radius: 4px; }\n",
    "</style>\n",
    "\n",
    "<div class=\"title\">ğŸ¦ Loan Approval Prediction Using Logistic Regression</div>\n",
    "\n",
    "<div class=\"subtitle\">ğŸ“Œ Project Overview</div>\n",
    "<p>\n",
    "    Loan approval is a critical process for financial institutions. To streamline decision-making, I am building a \n",
    "    <span class=\"highlight\">logistic regression model</span> that predicts whether a loan application will be approved based on key features such as:\n",
    "</p>\n",
    "\n",
    "<ul>\n",
    "    <li>âœ” <b>Applicant Income</b></li>\n",
    "    <li>âœ” <b>Coapplicant Income</b></li>\n",
    "    <li>âœ” <b>Loan Amount & Term</b></li>\n",
    "    <li>âœ” <b>Credit History</b></li>\n",
    "</ul>\n",
    "\n",
    "<div class=\"subtitle\">ğŸ¯ Objective</div>\n",
    "<p>\n",
    "    The goal is to develop a <b>data-driven model</b> that helps in making accurate loan approval predictions. \n",
    "    This project follows a structured workflow:\n",
    "</p>\n",
    "\n",
    "<ul>\n",
    "    <li>ğŸ”¹ <b>Data Cleaning & Preprocessing</b></li>\n",
    "    <li>ğŸ”¹ <b>Exploratory Data Analysis (EDA)</b></li>\n",
    "    <li>ğŸ”¹ <b>Feature Engineering</b></li>\n",
    "    <li>ğŸ”¹ <b>Model Building & Evaluation</b></li>\n",
    "</ul>\n",
    "\n",
    "<p>\n",
    "    By the end of this project, I aim to achieve a well-tuned logistic regression model that balances <b>accuracy</b> and <b>interpretability</b>.\n",
    "</p>\n",
    "\n",
    "<div class=\"subtitle\">ğŸ“‚ Dataset</div>\n",
    "<p>\n",
    "    The dataset used in this project can be accessed from \n",
    "    <a href=\"https://www.kaggle.com/datasets/ninzaami/loan-predication\" target=\"_blank\"><b>this Kaggle link</b></a>.\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“¥ Loading the Dataset  \n",
    "\n",
    "To begin, I load the **Loan Approval Dataset** into a Pandas DataFrame. The dataset contains **applicant details, income information, loan attributes, and approval status**. Hereâ€™s a glimpse of the data:\n",
    "\n",
    "ğŸ”¹ **Categorical Features**: Gender, Married, Education, Self-Employed, Property Area, Loan Status  \n",
    "ğŸ”¹ **Numerical Features**: Applicant Income, Coapplicant Income, Loan Amount, Loan Term, Credit History  \n",
    "\n",
    "Before proceeding, I'll perform **data cleaning** to handle missing values and inconsistencies. ğŸš€\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Loan_ID Gender Married Dependents     Education Self_Employed  \\\n",
      "0  LP001002   Male      No          0      Graduate            No   \n",
      "1  LP001003   Male     Yes          1      Graduate            No   \n",
      "2  LP001005   Male     Yes          0      Graduate           Yes   \n",
      "3  LP001006   Male     Yes          0  Not Graduate            No   \n",
      "4  LP001008   Male      No          0      Graduate            No   \n",
      "\n",
      "   ApplicantIncome  CoapplicantIncome  LoanAmount  Loan_Amount_Term  \\\n",
      "0             5849                0.0         NaN             360.0   \n",
      "1             4583             1508.0       128.0             360.0   \n",
      "2             3000                0.0        66.0             360.0   \n",
      "3             2583             2358.0       120.0             360.0   \n",
      "4             6000                0.0       141.0             360.0   \n",
      "\n",
      "   Credit_History Property_Area Loan_Status  \n",
      "0             1.0         Urban           Y  \n",
      "1             1.0         Rural           N  \n",
      "2             1.0         Urban           Y  \n",
      "3             1.0         Urban           Y  \n",
      "4             1.0         Urban           Y  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('loan_approval.csv') \n",
    "print(df.head()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ” Data Inspection & Summary\n",
    "\n",
    "Before diving deeper, it's important to examine the dataset's structure and check for any missing or outlier values. I'll first review:\n",
    "\n",
    "1. **Dataset Info** â€“ To check the data types and non-null counts.\n",
    "2. **Missing Values** â€“ To see if there are any missing entries in the columns.\n",
    "3. **Statistical Summary** â€“ To understand the distribution of numerical features.\n",
    "4. **Unique Values in 'Gender' Column** â€“ To check the categories of gender for potential encoding.\n",
    "\n",
    "This step will help me identify issues like missing values, incorrect data types, or outliers, which need to be addressed for further analysis. ğŸš€\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 614 entries, 0 to 613\n",
      "Data columns (total 13 columns):\n",
      " #   Column             Non-Null Count  Dtype  \n",
      "---  ------             --------------  -----  \n",
      " 0   Loan_ID            614 non-null    object \n",
      " 1   Gender             601 non-null    object \n",
      " 2   Married            611 non-null    object \n",
      " 3   Dependents         599 non-null    object \n",
      " 4   Education          614 non-null    object \n",
      " 5   Self_Employed      582 non-null    object \n",
      " 6   ApplicantIncome    614 non-null    int64  \n",
      " 7   CoapplicantIncome  614 non-null    float64\n",
      " 8   LoanAmount         592 non-null    float64\n",
      " 9   Loan_Amount_Term   600 non-null    float64\n",
      " 10  Credit_History     564 non-null    float64\n",
      " 11  Property_Area      614 non-null    object \n",
      " 12  Loan_Status        614 non-null    object \n",
      "dtypes: float64(4), int64(1), object(8)\n",
      "memory usage: 62.5+ KB\n",
      "None\n",
      "Loan_ID               0\n",
      "Gender               13\n",
      "Married               3\n",
      "Dependents           15\n",
      "Education             0\n",
      "Self_Employed        32\n",
      "ApplicantIncome       0\n",
      "CoapplicantIncome     0\n",
      "LoanAmount           22\n",
      "Loan_Amount_Term     14\n",
      "Credit_History       50\n",
      "Property_Area         0\n",
      "Loan_Status           0\n",
      "dtype: int64\n",
      "       ApplicantIncome  CoapplicantIncome  LoanAmount  Loan_Amount_Term  \\\n",
      "count       614.000000         614.000000  592.000000         600.00000   \n",
      "mean       5403.459283        1621.245798  146.412162         342.00000   \n",
      "std        6109.041673        2926.248369   85.587325          65.12041   \n",
      "min         150.000000           0.000000    9.000000          12.00000   \n",
      "25%        2877.500000           0.000000  100.000000         360.00000   \n",
      "50%        3812.500000        1188.500000  128.000000         360.00000   \n",
      "75%        5795.000000        2297.250000  168.000000         360.00000   \n",
      "max       81000.000000       41667.000000  700.000000         480.00000   \n",
      "\n",
      "       Credit_History  \n",
      "count      564.000000  \n",
      "mean         0.842199  \n",
      "std          0.364878  \n",
      "min          0.000000  \n",
      "25%          1.000000  \n",
      "50%          1.000000  \n",
      "75%          1.000000  \n",
      "max          1.000000  \n",
      "['Male' 'Female' nan]\n"
     ]
    }
   ],
   "source": [
    "print(df.info())\n",
    "print(df.isnull().sum())\n",
    "print(df.describe())\n",
    "print(df['Gender'].unique())  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ§¹ Handling Missing Data  \n",
    "\n",
    "The dataset contains several missing values across different columns. To ensure no missing values impact the model's performance, I'll fill these missing values using the most appropriate method:\n",
    "\n",
    "- **Numerical columns like 'LoanAmount'** will be filled with the **median** value to avoid skewing the data.\n",
    "- **Categorical columns like 'Gender', 'Married', 'Dependents', 'Self_Employed', and 'Credit_History'** will be filled with the **mode** (most frequent value) to maintain the category distribution.\n",
    "\n",
    "After filling in the missing values, I'll verify that no more missing data exists in the dataset. This ensures that the model will not encounter any issues when training.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loan_ID               0\n",
      "Gender               13\n",
      "Married               3\n",
      "Dependents           15\n",
      "Education             0\n",
      "Self_Employed        32\n",
      "ApplicantIncome       0\n",
      "CoapplicantIncome     0\n",
      "LoanAmount           22\n",
      "Loan_Amount_Term     14\n",
      "Credit_History       50\n",
      "Property_Area         0\n",
      "Loan_Status           0\n",
      "dtype: int64\n",
      "Loan_ID              0\n",
      "Gender               0\n",
      "Married              0\n",
      "Dependents           0\n",
      "Education            0\n",
      "Self_Employed        0\n",
      "ApplicantIncome      0\n",
      "CoapplicantIncome    0\n",
      "LoanAmount           0\n",
      "Loan_Amount_Term     0\n",
      "Credit_History       0\n",
      "Property_Area        0\n",
      "Loan_Status          0\n",
      "dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MoaviaHassan\\AppData\\Local\\Temp\\ipykernel_10576\\963934493.py:3: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['LoanAmount'].fillna(df['LoanAmount'].median(), inplace=True)\n",
      "C:\\Users\\MoaviaHassan\\AppData\\Local\\Temp\\ipykernel_10576\\963934493.py:4: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['Gender'].fillna(df['Gender'].mode()[0], inplace=True)\n",
      "C:\\Users\\MoaviaHassan\\AppData\\Local\\Temp\\ipykernel_10576\\963934493.py:5: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['Married'].fillna(df['Married'].mode()[0], inplace=True)\n",
      "C:\\Users\\MoaviaHassan\\AppData\\Local\\Temp\\ipykernel_10576\\963934493.py:6: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['Dependents'].fillna(df['Dependents'].mode()[0], inplace=True)\n",
      "C:\\Users\\MoaviaHassan\\AppData\\Local\\Temp\\ipykernel_10576\\963934493.py:7: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['Self_Employed'].fillna(df['Self_Employed'].mode()[0], inplace=True)\n",
      "C:\\Users\\MoaviaHassan\\AppData\\Local\\Temp\\ipykernel_10576\\963934493.py:8: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['Credit_History'].fillna(df['Credit_History'].mode()[0], inplace=True)\n",
      "C:\\Users\\MoaviaHassan\\AppData\\Local\\Temp\\ipykernel_10576\\963934493.py:9: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['Loan_Amount_Term'].fillna(df['Loan_Amount_Term'].mode()[0], inplace=True)\n"
     ]
    }
   ],
   "source": [
    "print(df.isnull().sum())\n",
    "\n",
    "df['LoanAmount'].fillna(df['LoanAmount'].median(), inplace=True)\n",
    "df['Gender'].fillna(df['Gender'].mode()[0], inplace=True)\n",
    "df['Married'].fillna(df['Married'].mode()[0], inplace=True)\n",
    "df['Dependents'].fillna(df['Dependents'].mode()[0], inplace=True)\n",
    "df['Self_Employed'].fillna(df['Self_Employed'].mode()[0], inplace=True)\n",
    "df['Credit_History'].fillna(df['Credit_History'].mode()[0], inplace=True)\n",
    "df['Loan_Amount_Term'].fillna(df['Loan_Amount_Term'].mode()[0], inplace=True)\n",
    "print(df.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸš¨ Handling Outliers in 'LoanAmount'\n",
    "\n",
    "Outliers can significantly impact the performance of machine learning models, especially in regression tasks. Therefore, I will address outliers in the **'LoanAmount'** column using the **Interquartile Range (IQR)** method.\n",
    "\n",
    "The steps involved:\n",
    "1. **Calculate the first (Q1) and third (Q3) quartiles** to determine the IQR.\n",
    "2. **Define the lower and upper bounds** for normal values.\n",
    "3. **Cap values** that fall outside the bounds to the closest limit.\n",
    "\n",
    "This method ensures that extreme outliers are treated while keeping the distribution intact.\n",
    "\n",
    "After applying this, I'll check the distribution of 'LoanAmount' again to confirm the adjustments. ğŸ“Š\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    614.000000\n",
      "mean     137.365635\n",
      "std       55.779749\n",
      "min        9.000000\n",
      "25%      100.250000\n",
      "50%      128.000000\n",
      "75%      164.750000\n",
      "max      261.500000\n",
      "Name: LoanAmount, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "Q1 = df['LoanAmount'].quantile(0.25)\n",
    "Q3 = df['LoanAmount'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "df['LoanAmount'] = df['LoanAmount'].apply(lambda x: upper_bound if x > upper_bound else (lower_bound if x < lower_bound else x))\n",
    "\n",
    "print(df['LoanAmount'].describe())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ”¢ Encoding Categorical Variables\n",
    "\n",
    "To ensure the machine learning model can process categorical features, I will **encode** them into numerical format. This is done using **one-hot encoding**, which creates binary columns for each category. For efficiency, I will **drop the first category** in each column to avoid multicollinearity (the \"dummy variable trap\").\n",
    "\n",
    "Columns to be encoded:\n",
    "- **'Gender'**, **'Married'**, **'Education'**, **'Self_Employed'**, and **'Property_Area'**\n",
    "\n",
    "By applying this transformation, the model will be able to interpret categorical variables as numerical input without any assumptions of ordinal relationships.  \n",
    "\n",
    "After encoding, I will check the updated dataset to confirm the changes. ğŸ“Š\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Loan_ID Dependents  ApplicantIncome  CoapplicantIncome  LoanAmount  \\\n",
      "0  LP001002          0             5849                0.0       128.0   \n",
      "1  LP001003          1             4583             1508.0       128.0   \n",
      "2  LP001005          0             3000                0.0        66.0   \n",
      "3  LP001006          0             2583             2358.0       120.0   \n",
      "4  LP001008          0             6000                0.0       141.0   \n",
      "\n",
      "   Loan_Amount_Term  Credit_History Loan_Status  Gender_Male  Married_Yes  \\\n",
      "0             360.0             1.0           Y         True        False   \n",
      "1             360.0             1.0           N         True         True   \n",
      "2             360.0             1.0           Y         True         True   \n",
      "3             360.0             1.0           Y         True         True   \n",
      "4             360.0             1.0           Y         True        False   \n",
      "\n",
      "   Education_Not Graduate  Self_Employed_Yes  Property_Area_Semiurban  \\\n",
      "0                   False              False                    False   \n",
      "1                   False              False                    False   \n",
      "2                   False               True                    False   \n",
      "3                    True              False                    False   \n",
      "4                   False              False                    False   \n",
      "\n",
      "   Property_Area_Urban  \n",
      "0                 True  \n",
      "1                False  \n",
      "2                 True  \n",
      "3                 True  \n",
      "4                 True  \n"
     ]
    }
   ],
   "source": [
    "df = pd.get_dummies(df, columns=['Gender', 'Married', 'Education', 'Self_Employed', 'Property_Area'], drop_first=True)\n",
    "\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ’¾ Saving the Cleaned Dataset\n",
    "\n",
    "After handling missing values, outliers, and encoding categorical variables, the dataset is now ready for use in training models. To preserve these changes, I will **save the cleaned dataset** to a new CSV file called **'cleaned_loan_prediction.csv'**. This will allow me to easily load the dataset for future analysis or model training.\n",
    "\n",
    "The file will not include the index, keeping the structure neat and consistent with the original data.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('cleaned_loan_prediction.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“Š Exploratory Data Analysis (EDA)\n",
    "\n",
    "Now that the data is cleaned, it's important to **perform some initial exploratory analysis** to understand the dataset better.\n",
    "\n",
    "1. **Descriptive Statistics**: I will start by looking at the summary statistics for numerical columns, such as **ApplicantIncome**, **LoanAmount**, and **Credit_History**. This helps in understanding the central tendency, spread, and any potential anomalies.\n",
    "   \n",
    "2. **Frequency Distribution**: I will also display the frequency distribution for categorical columns, specifically the **'Loan_Status'** (target variable) and **'Credit_History'**, to see how balanced the target is and the distribution of other key features.\n",
    "\n",
    "### Key Observations:\n",
    "- The **Loan_Status** column shows that there are more **'Y'** (approved) loans compared to **'N'** (not approved), which gives an insight into the data's balance.\n",
    "- The **Credit_History** column mostly has values of 1.0 (indicating a positive credit history), with a small number of 0.0 entries (indicating a negative credit history).\n",
    "\n",
    "This analysis gives a good foundation for deciding the next steps in model building. ğŸš€\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       ApplicantIncome  CoapplicantIncome  LoanAmount  Loan_Amount_Term  \\\n",
      "count       614.000000         614.000000  614.000000        614.000000   \n",
      "mean       5403.459283        1621.245798  137.365635        342.410423   \n",
      "std        6109.041673        2926.248369   55.779749         64.428629   \n",
      "min         150.000000           0.000000    9.000000         12.000000   \n",
      "25%        2877.500000           0.000000  100.250000        360.000000   \n",
      "50%        3812.500000        1188.500000  128.000000        360.000000   \n",
      "75%        5795.000000        2297.250000  164.750000        360.000000   \n",
      "max       81000.000000       41667.000000  261.500000        480.000000   \n",
      "\n",
      "       Credit_History  \n",
      "count      614.000000  \n",
      "mean         0.855049  \n",
      "std          0.352339  \n",
      "min          0.000000  \n",
      "25%          1.000000  \n",
      "50%          1.000000  \n",
      "75%          1.000000  \n",
      "max          1.000000  \n",
      "Loan_Status\n",
      "Y    422\n",
      "N    192\n",
      "Name: count, dtype: int64\n",
      "Credit_History\n",
      "1.0    525\n",
      "0.0     89\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('cleaned_loan_prediction.csv')\n",
    "\n",
    "print(df.describe())\n",
    "\n",
    "print(df['Loan_Status'].value_counts())  # Target variable\n",
    "print(df['Credit_History'].value_counts())  # Example for another categorical column\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“Š Statistical Testing: Chi-square Test\n",
    "\n",
    "Next, I will perform a **Chi-square test** to analyze the relationship between **Credit_History** (a categorical feature) and **Loan_Status** (the target variable). The goal is to assess whether there is a significant association between the applicant's credit history and the loan approval status.\n",
    "\n",
    "### **Chi-square Test Results:**\n",
    "- **Chi2 Statistic**: 176.11 â€” This is the test statistic calculated from the observed and expected frequencies.\n",
    "- **P-value**: 3.42e-40 â€” A very low p-value suggests strong evidence against the null hypothesis, meaning there is a statistically significant relationship between **Credit_History** and **Loan_Status**.\n",
    "- **Degrees of Freedom**: 1 â€” The degrees of freedom for this test, calculated as the number of categories in **Credit_History** minus one, multiplied by the number of categories in **Loan_Status** minus one.\n",
    "- **Expected Frequencies**: A table showing the expected counts for each combination of categories.\n",
    "\n",
    "Since the p-value is very small (less than 0.05), I can confidently reject the null hypothesis, concluding that **Credit_History** has a significant influence on **Loan_Status**.\n",
    "\n",
    "This statistical test helps provide insight into which features are worth considering in the model. ğŸš€\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chi-square Test Results:\n",
      "Chi2 Statistic: 176.1145746235241\n",
      "P-value: 3.4183499979091188e-40\n",
      "Degrees of Freedom: 1\n",
      "Expected Frequencies Table:\n",
      "[[ 27.83061889  61.16938111]\n",
      " [164.16938111 360.83061889]]\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "contingency_table = pd.crosstab(df['Credit_History'], df['Loan_Status'])\n",
    "\n",
    "chi2, p_value, dof, expected = chi2_contingency(contingency_table)\n",
    "\n",
    "print(\"Chi-square Test Results:\")\n",
    "print(f\"Chi2 Statistic: {chi2}\")\n",
    "print(f\"P-value: {p_value}\")\n",
    "print(f\"Degrees of Freedom: {dof}\")\n",
    "print(\"Expected Frequencies Table:\")\n",
    "print(expected)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ§‘â€ğŸ’¼ T-test for Independent Samples: ApplicantIncome\n",
    "\n",
    "Now, I will conduct a **t-test** to compare the **ApplicantIncome** between the approved and rejected loan groups. The t-test helps assess whether there is a significant difference in the mean income between the two groups.\n",
    "\n",
    "### **T-test Results for ApplicantIncome:**\n",
    "- **t-statistic**: -0.1165 â€” The t-statistic is close to zero, indicating a minimal difference between the two groups.\n",
    "- **p-value**: 0.9073 â€” Since the p-value is much greater than 0.05, I fail to reject the null hypothesis. This suggests that there is **no significant difference** in the average income between approved and rejected loan applicants.\n",
    "\n",
    "### **Conclusion:**\n",
    "The lack of a significant result implies that **ApplicantIncome** alone may not be a strong predictor for loan approval, which might suggest considering other features or transforming this feature for better model performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t-test Results for ApplicantIncome:\n",
      "t-statistic: -0.11650844828724542\n",
      "p-value: 0.907287812130518\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import ttest_ind\n",
    "\n",
    "income_approved = df[df['Loan_Status'] == 'Y']['ApplicantIncome']\n",
    "income_rejected = df[df['Loan_Status'] == 'N']['ApplicantIncome']\n",
    "\n",
    "t_stat, p_value = ttest_ind(income_approved, income_rejected)\n",
    "\n",
    "print(\"t-test Results for ApplicantIncome:\")\n",
    "print(f\"t-statistic: {t_stat}\")\n",
    "print(f\"p-value: {p_value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variance Inflation Factor (VIF) Calculation\n",
    "\n",
    "In this step, I calculate the Variance Inflation Factor (VIF) for each independent variable. VIF measures the extent to which the variance of an estimated regression coefficient increases due to collinearity with other predictors. A higher VIF indicates high multicollinearity.\n",
    "\n",
    "Here, I focus on the following variables: \n",
    "- `ApplicantIncome`\n",
    "- `CoapplicantIncome`\n",
    "- `LoanAmount`\n",
    "- `Loan_Amount_Term`\n",
    "- `Credit_History`\n",
    "\n",
    "Typically, a VIF above 5-10 suggests problematic multicollinearity, which might affect the modelâ€™s accuracy. If necessary, I will consider transforming or removing variables with high VIF values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VIF Results:\n",
      "            Variable       VIF\n",
      "0    ApplicantIncome  2.321698\n",
      "1  CoapplicantIncome  1.456320\n",
      "2         LoanAmount  9.008554\n",
      "3   Loan_Amount_Term  9.639508\n",
      "4     Credit_History  5.871926\n"
     ]
    }
   ],
   "source": [
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "independent_vars = df[['ApplicantIncome', 'CoapplicantIncome', 'LoanAmount', 'Loan_Amount_Term', 'Credit_History']]\n",
    "\n",
    "vif_data = pd.DataFrame()\n",
    "vif_data['Variable'] = independent_vars.columns\n",
    "vif_data['VIF'] = [variance_inflation_factor(independent_vars.values, i) for i in range(independent_vars.shape[1])]\n",
    "\n",
    "print(\"VIF Results:\")\n",
    "print(vif_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling Multicollinearity by Removing Highly Correlated Variables\n",
    "\n",
    "After analyzing the Variance Inflation Factor (VIF) values, it was observed that `LoanAmount` and `Loan_Amount_Term` have high VIF, indicating multicollinearity. To address this, we will remove the `Loan_Amount_Term` variable and recalculate the VIF for the remaining features. By doing this, we reduce multicollinearity and make the model more stable.\n",
    "\n",
    "#### Code to Remove `Loan_Amount_Term` and Recalculate VIF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated VIF Results after removing Loan_Amount_Term:\n",
      "            Variable       VIF\n",
      "0    ApplicantIncome  2.303778\n",
      "1  CoapplicantIncome  1.450722\n",
      "2         LoanAmount  5.975299\n",
      "3     Credit_History  3.791828\n"
     ]
    }
   ],
   "source": [
    "df = df.drop(columns=['Loan_Amount_Term'])\n",
    "\n",
    "independent_vars = df[['ApplicantIncome', 'CoapplicantIncome', 'LoanAmount', 'Credit_History']]\n",
    "vif_data = pd.DataFrame()\n",
    "vif_data['Variable'] = independent_vars.columns\n",
    "vif_data['VIF'] = [variance_inflation_factor(independent_vars.values, i) for i in range(independent_vars.shape[1])]\n",
    "\n",
    "print(\"Updated VIF Results after removing Loan_Amount_Term:\")\n",
    "print(vif_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding \"Monthly Loan Payment\" Feature\n",
    "\n",
    "We will add a new feature **\"Monthly Loan Payment\"** based on a standard loan payment formula. The formula for monthly payments is:\n",
    "\n",
    "\\[\n",
    "M = \\frac{P \\times r \\times (1 + r)^n}{(1 + r)^n - 1}\n",
    "\\]\n",
    "\n",
    "Where:\n",
    "- **M** is the monthly payment.\n",
    "- **P** is the loan amount.\n",
    "- **r** is the monthly interest rate (annual rate divided by 12).\n",
    "- **n** is the number of payments (loan term in months).\n",
    "\n",
    "We will also handle outliers in the newly created feature to ensure its validity.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   LoanAmount  Monthly_Loan_Payment\n",
      "0       128.0              1.123292\n",
      "1       128.0              1.123292\n",
      "2        66.0              0.579197\n",
      "3       120.0              1.053086\n",
      "4       141.0              1.237376\n",
      "Cleaned Monthly Loan Payment Feature:\n",
      "0    1.123292\n",
      "1    1.123292\n",
      "2    0.579197\n",
      "3    1.053086\n",
      "4    1.237376\n",
      "Name: Monthly_Loan_Payment, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "annual_interest_rate = 0.10\n",
    "monthly_interest_rate = annual_interest_rate / 12\n",
    "loan_term = 360\n",
    "\n",
    "df['Monthly_Loan_Payment'] = df['LoanAmount'] * monthly_interest_rate * (1 + monthly_interest_rate) ** loan_term / ((1 + monthly_interest_rate) ** loan_term - 1)\n",
    "\n",
    "print(df[['LoanAmount', 'Monthly_Loan_Payment']].head())\n",
    "\n",
    "Q1 = df['Monthly_Loan_Payment'].quantile(0.25)\n",
    "Q3 = df['Monthly_Loan_Payment'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "df['Monthly_Loan_Payment'] = df['Monthly_Loan_Payment'].apply(lambda x: upper_bound if x > upper_bound else (lower_bound if x < lower_bound else x))\n",
    "\n",
    "print(\"Cleaned Monthly Loan Payment Feature:\")\n",
    "print(df['Monthly_Loan_Payment'].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding the \"Monthly Loan Payment\" feature and Handling Outliers\n",
    "\n",
    "In this step, we will create a new feature \"Monthly Loan Payment\" based on the formula for monthly payments of a loan. We'll assume a fixed interest rate and a standard loan term of 30 years (360 months).\n",
    "\n",
    "Then, we'll handle outliers for the \"Monthly Loan Payment\" feature by applying the Interquartile Range (IQR) method to cap the values within the upper and lower bounds.\n",
    "\n",
    "The formula for the monthly payment is:\n",
    "\n",
    "\\[\n",
    "M = \\frac{P \\cdot r \\cdot (1 + r)^n}{(1 + r)^n - 1}\n",
    "\\]\n",
    "\n",
    "Where:\n",
    "- \\( P \\) is the loan amount\n",
    "- \\( r \\) is the monthly interest rate (annual rate / 12)\n",
    "- \\( n \\) is the loan term in months (360 months for a 30-year term)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   LoanAmount  Monthly_Loan_Payment\n",
      "0       128.0              1.123292\n",
      "1       128.0              1.123292\n",
      "2        66.0              0.579197\n",
      "3       120.0              1.053086\n",
      "4       141.0              1.237376\n"
     ]
    }
   ],
   "source": [
    "annual_interest_rate = 0.1\n",
    "r = annual_interest_rate / 12\n",
    "n = 360\n",
    "\n",
    "df['Monthly_Loan_Payment'] = (df['LoanAmount'] * r * (1 + r)**n) / ((1 + r)**n - 1)\n",
    "\n",
    "Q1 = df['Monthly_Loan_Payment'].quantile(0.25)\n",
    "Q3 = df['Monthly_Loan_Payment'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "df['Monthly_Loan_Payment'] = df['Monthly_Loan_Payment'].apply(lambda x: upper_bound if x > upper_bound else (lower_bound if x < lower_bound else x))\n",
    "\n",
    "print(df[['LoanAmount', 'Monthly_Loan_Payment']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    1\n",
      "1    0\n",
      "2    1\n",
      "3    1\n",
      "4    1\n",
      "Name: Loan_Status, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df['Loan_Status'] = df['Loan_Status'].map({'Y': 1, 'N': 0})\n",
    "\n",
    "print(df['Loan_Status'].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting the Data into Training and Testing Sets\n",
    "\n",
    "Now that we have preprocessed and encoded the data, the next step is to split it into training and testing sets. This will allow us to train our machine learning model on one subset of the data and evaluate its performance on another.\n",
    "\n",
    "We'll use the `train_test_split` function from `sklearn.model_selection` to split the data, with 80% for training and 20% for testing.\n",
    "\n",
    "```python\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df.drop(columns=['Loan_Status'])\n",
    "y = df['Loan_Status']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"Training set size: {X_train.shape[0]}\")\n",
    "print(f\"Testing set size: {X_test.shape[0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First few rows of the dataframe:\n",
      "    Loan_ID Dependents  ApplicantIncome  CoapplicantIncome  LoanAmount  \\\n",
      "0  LP001002          0             5849                0.0       128.0   \n",
      "1  LP001003          1             4583             1508.0       128.0   \n",
      "2  LP001005          0             3000                0.0        66.0   \n",
      "3  LP001006          0             2583             2358.0       120.0   \n",
      "4  LP001008          0             6000                0.0       141.0   \n",
      "\n",
      "   Credit_History  Loan_Status  Gender_Male  Married_Yes  \\\n",
      "0             1.0            1         True        False   \n",
      "1             1.0            0         True         True   \n",
      "2             1.0            1         True         True   \n",
      "3             1.0            1         True         True   \n",
      "4             1.0            1         True        False   \n",
      "\n",
      "   Education_Not Graduate  Self_Employed_Yes  Property_Area_Semiurban  \\\n",
      "0                   False              False                    False   \n",
      "1                   False              False                    False   \n",
      "2                   False               True                    False   \n",
      "3                    True              False                    False   \n",
      "4                   False              False                    False   \n",
      "\n",
      "   Property_Area_Urban  Monthly_Loan_Payment  \n",
      "0                 True              1.123292  \n",
      "1                False              1.123292  \n",
      "2                 True              0.579197  \n",
      "3                 True              1.053086  \n",
      "4                 True              1.237376  \n",
      "\n",
      "Missing values in each column:\n",
      "Loan_ID                    0\n",
      "Dependents                 0\n",
      "ApplicantIncome            0\n",
      "CoapplicantIncome          0\n",
      "LoanAmount                 0\n",
      "Credit_History             0\n",
      "Loan_Status                0\n",
      "Gender_Male                0\n",
      "Married_Yes                0\n",
      "Education_Not Graduate     0\n",
      "Self_Employed_Yes          0\n",
      "Property_Area_Semiurban    0\n",
      "Property_Area_Urban        0\n",
      "Monthly_Loan_Payment       0\n",
      "dtype: int64\n",
      "\n",
      "Data types of each column:\n",
      "Loan_ID                     object\n",
      "Dependents                  object\n",
      "ApplicantIncome              int64\n",
      "CoapplicantIncome          float64\n",
      "LoanAmount                 float64\n",
      "Credit_History             float64\n",
      "Loan_Status                  int64\n",
      "Gender_Male                   bool\n",
      "Married_Yes                   bool\n",
      "Education_Not Graduate        bool\n",
      "Self_Employed_Yes             bool\n",
      "Property_Area_Semiurban       bool\n",
      "Property_Area_Urban           bool\n",
      "Monthly_Loan_Payment       float64\n",
      "dtype: object\n",
      "\n",
      "Unique values in each column (for non-numeric checks):\n",
      "Loan_ID: ['LP001002' 'LP001003' 'LP001005' 'LP001006' 'LP001008' 'LP001011'\n",
      " 'LP001013' 'LP001014' 'LP001018' 'LP001020' 'LP001024' 'LP001027'\n",
      " 'LP001028' 'LP001029' 'LP001030' 'LP001032' 'LP001034' 'LP001036'\n",
      " 'LP001038' 'LP001041' 'LP001043' 'LP001046' 'LP001047' 'LP001050'\n",
      " 'LP001052' 'LP001066' 'LP001068' 'LP001073' 'LP001086' 'LP001087'\n",
      " 'LP001091' 'LP001095' 'LP001097' 'LP001098' 'LP001100' 'LP001106'\n",
      " 'LP001109' 'LP001112' 'LP001114' 'LP001116' 'LP001119' 'LP001120'\n",
      " 'LP001123' 'LP001131' 'LP001136' 'LP001137' 'LP001138' 'LP001144'\n",
      " 'LP001146' 'LP001151' 'LP001155' 'LP001157' 'LP001164' 'LP001179'\n",
      " 'LP001186' 'LP001194' 'LP001195' 'LP001197' 'LP001198' 'LP001199'\n",
      " 'LP001205' 'LP001206' 'LP001207' 'LP001213' 'LP001222' 'LP001225'\n",
      " 'LP001228' 'LP001233' 'LP001238' 'LP001241' 'LP001243' 'LP001245'\n",
      " 'LP001248' 'LP001250' 'LP001253' 'LP001255' 'LP001256' 'LP001259'\n",
      " 'LP001263' 'LP001264' 'LP001265' 'LP001266' 'LP001267' 'LP001273'\n",
      " 'LP001275' 'LP001279' 'LP001280' 'LP001282' 'LP001289' 'LP001310'\n",
      " 'LP001316' 'LP001318' 'LP001319' 'LP001322' 'LP001325' 'LP001326'\n",
      " 'LP001327' 'LP001333' 'LP001334' 'LP001343' 'LP001345' 'LP001349'\n",
      " 'LP001350' 'LP001356' 'LP001357' 'LP001367' 'LP001369' 'LP001370'\n",
      " 'LP001379' 'LP001384' 'LP001385' 'LP001387' 'LP001391' 'LP001392'\n",
      " 'LP001398' 'LP001401' 'LP001404' 'LP001405' 'LP001421' 'LP001422'\n",
      " 'LP001426' 'LP001430' 'LP001431' 'LP001432' 'LP001439' 'LP001443'\n",
      " 'LP001448' 'LP001449' 'LP001451' 'LP001465' 'LP001469' 'LP001473'\n",
      " 'LP001478' 'LP001482' 'LP001487' 'LP001488' 'LP001489' 'LP001491'\n",
      " 'LP001492' 'LP001493' 'LP001497' 'LP001498' 'LP001504' 'LP001507'\n",
      " 'LP001508' 'LP001514' 'LP001516' 'LP001518' 'LP001519' 'LP001520'\n",
      " 'LP001528' 'LP001529' 'LP001531' 'LP001532' 'LP001535' 'LP001536'\n",
      " 'LP001541' 'LP001543' 'LP001546' 'LP001552' 'LP001560' 'LP001562'\n",
      " 'LP001565' 'LP001570' 'LP001572' 'LP001574' 'LP001577' 'LP001578'\n",
      " 'LP001579' 'LP001580' 'LP001581' 'LP001585' 'LP001586' 'LP001594'\n",
      " 'LP001603' 'LP001606' 'LP001608' 'LP001610' 'LP001616' 'LP001630'\n",
      " 'LP001633' 'LP001634' 'LP001636' 'LP001637' 'LP001639' 'LP001640'\n",
      " 'LP001641' 'LP001643' 'LP001644' 'LP001647' 'LP001653' 'LP001656'\n",
      " 'LP001657' 'LP001658' 'LP001664' 'LP001665' 'LP001666' 'LP001669'\n",
      " 'LP001671' 'LP001673' 'LP001674' 'LP001677' 'LP001682' 'LP001688'\n",
      " 'LP001691' 'LP001692' 'LP001693' 'LP001698' 'LP001699' 'LP001702'\n",
      " 'LP001708' 'LP001711' 'LP001713' 'LP001715' 'LP001716' 'LP001720'\n",
      " 'LP001722' 'LP001726' 'LP001732' 'LP001734' 'LP001736' 'LP001743'\n",
      " 'LP001744' 'LP001749' 'LP001750' 'LP001751' 'LP001754' 'LP001758'\n",
      " 'LP001760' 'LP001761' 'LP001765' 'LP001768' 'LP001770' 'LP001776'\n",
      " 'LP001778' 'LP001784' 'LP001786' 'LP001788' 'LP001790' 'LP001792'\n",
      " 'LP001798' 'LP001800' 'LP001806' 'LP001807' 'LP001811' 'LP001813'\n",
      " 'LP001814' 'LP001819' 'LP001824' 'LP001825' 'LP001835' 'LP001836'\n",
      " 'LP001841' 'LP001843' 'LP001844' 'LP001846' 'LP001849' 'LP001854'\n",
      " 'LP001859' 'LP001864' 'LP001865' 'LP001868' 'LP001870' 'LP001871'\n",
      " 'LP001872' 'LP001875' 'LP001877' 'LP001882' 'LP001883' 'LP001884'\n",
      " 'LP001888' 'LP001891' 'LP001892' 'LP001894' 'LP001896' 'LP001900'\n",
      " 'LP001903' 'LP001904' 'LP001907' 'LP001908' 'LP001910' 'LP001914'\n",
      " 'LP001915' 'LP001917' 'LP001922' 'LP001924' 'LP001925' 'LP001926'\n",
      " 'LP001931' 'LP001935' 'LP001936' 'LP001938' 'LP001940' 'LP001945'\n",
      " 'LP001947' 'LP001949' 'LP001953' 'LP001954' 'LP001955' 'LP001963'\n",
      " 'LP001964' 'LP001972' 'LP001974' 'LP001977' 'LP001978' 'LP001990'\n",
      " 'LP001993' 'LP001994' 'LP001996' 'LP001998' 'LP002002' 'LP002004'\n",
      " 'LP002006' 'LP002008' 'LP002024' 'LP002031' 'LP002035' 'LP002036'\n",
      " 'LP002043' 'LP002050' 'LP002051' 'LP002053' 'LP002054' 'LP002055'\n",
      " 'LP002065' 'LP002067' 'LP002068' 'LP002082' 'LP002086' 'LP002087'\n",
      " 'LP002097' 'LP002098' 'LP002100' 'LP002101' 'LP002103' 'LP002106'\n",
      " 'LP002110' 'LP002112' 'LP002113' 'LP002114' 'LP002115' 'LP002116'\n",
      " 'LP002119' 'LP002126' 'LP002128' 'LP002129' 'LP002130' 'LP002131'\n",
      " 'LP002137' 'LP002138' 'LP002139' 'LP002140' 'LP002141' 'LP002142'\n",
      " 'LP002143' 'LP002144' 'LP002149' 'LP002151' 'LP002158' 'LP002160'\n",
      " 'LP002161' 'LP002170' 'LP002175' 'LP002178' 'LP002180' 'LP002181'\n",
      " 'LP002187' 'LP002188' 'LP002190' 'LP002191' 'LP002194' 'LP002197'\n",
      " 'LP002201' 'LP002205' 'LP002209' 'LP002211' 'LP002219' 'LP002223'\n",
      " 'LP002224' 'LP002225' 'LP002226' 'LP002229' 'LP002231' 'LP002234'\n",
      " 'LP002236' 'LP002237' 'LP002239' 'LP002243' 'LP002244' 'LP002250'\n",
      " 'LP002255' 'LP002262' 'LP002263' 'LP002265' 'LP002266' 'LP002272'\n",
      " 'LP002277' 'LP002281' 'LP002284' 'LP002287' 'LP002288' 'LP002296'\n",
      " 'LP002297' 'LP002300' 'LP002301' 'LP002305' 'LP002308' 'LP002314'\n",
      " 'LP002315' 'LP002317' 'LP002318' 'LP002319' 'LP002328' 'LP002332'\n",
      " 'LP002335' 'LP002337' 'LP002341' 'LP002342' 'LP002345' 'LP002347'\n",
      " 'LP002348' 'LP002357' 'LP002361' 'LP002362' 'LP002364' 'LP002366'\n",
      " 'LP002367' 'LP002368' 'LP002369' 'LP002370' 'LP002377' 'LP002379'\n",
      " 'LP002386' 'LP002387' 'LP002390' 'LP002393' 'LP002398' 'LP002401'\n",
      " 'LP002403' 'LP002407' 'LP002408' 'LP002409' 'LP002418' 'LP002422'\n",
      " 'LP002424' 'LP002429' 'LP002434' 'LP002435' 'LP002443' 'LP002444'\n",
      " 'LP002446' 'LP002447' 'LP002448' 'LP002449' 'LP002453' 'LP002455'\n",
      " 'LP002459' 'LP002467' 'LP002472' 'LP002473' 'LP002478' 'LP002484'\n",
      " 'LP002487' 'LP002489' 'LP002493' 'LP002494' 'LP002500' 'LP002501'\n",
      " 'LP002502' 'LP002505' 'LP002515' 'LP002517' 'LP002519' 'LP002522'\n",
      " 'LP002524' 'LP002527' 'LP002529' 'LP002530' 'LP002531' 'LP002533'\n",
      " 'LP002534' 'LP002536' 'LP002537' 'LP002541' 'LP002543' 'LP002544'\n",
      " 'LP002545' 'LP002547' 'LP002555' 'LP002556' 'LP002560' 'LP002562'\n",
      " 'LP002571' 'LP002582' 'LP002585' 'LP002586' 'LP002587' 'LP002588'\n",
      " 'LP002600' 'LP002602' 'LP002603' 'LP002606' 'LP002615' 'LP002618'\n",
      " 'LP002619' 'LP002622' 'LP002624' 'LP002625' 'LP002626' 'LP002634'\n",
      " 'LP002637' 'LP002640' 'LP002643' 'LP002648' 'LP002652' 'LP002659'\n",
      " 'LP002670' 'LP002682' 'LP002683' 'LP002684' 'LP002689' 'LP002690'\n",
      " 'LP002692' 'LP002693' 'LP002697' 'LP002699' 'LP002705' 'LP002706'\n",
      " 'LP002714' 'LP002716' 'LP002717' 'LP002720' 'LP002723' 'LP002729'\n",
      " 'LP002731' 'LP002732' 'LP002734' 'LP002738' 'LP002739' 'LP002740'\n",
      " 'LP002741' 'LP002743' 'LP002753' 'LP002755' 'LP002757' 'LP002767'\n",
      " 'LP002768' 'LP002772' 'LP002776' 'LP002777' 'LP002778' 'LP002784'\n",
      " 'LP002785' 'LP002788' 'LP002789' 'LP002792' 'LP002794' 'LP002795'\n",
      " 'LP002798' 'LP002804' 'LP002807' 'LP002813' 'LP002820' 'LP002821'\n",
      " 'LP002832' 'LP002833' 'LP002836' 'LP002837' 'LP002840' 'LP002841'\n",
      " 'LP002842' 'LP002847' 'LP002855' 'LP002862' 'LP002863' 'LP002868'\n",
      " 'LP002872' 'LP002874' 'LP002877' 'LP002888' 'LP002892' 'LP002893'\n",
      " 'LP002894' 'LP002898' 'LP002911' 'LP002912' 'LP002916' 'LP002917'\n",
      " 'LP002925' 'LP002926' 'LP002928' 'LP002931' 'LP002933' 'LP002936'\n",
      " 'LP002938' 'LP002940' 'LP002941' 'LP002943' 'LP002945' 'LP002948'\n",
      " 'LP002949' 'LP002950' 'LP002953' 'LP002958' 'LP002959' 'LP002960'\n",
      " 'LP002961' 'LP002964' 'LP002974' 'LP002978' 'LP002979' 'LP002983'\n",
      " 'LP002984' 'LP002990']\n",
      "Dependents: ['0' '1' '2' '3+']\n"
     ]
    }
   ],
   "source": [
    "# Display the first few rows of the dataframe to inspect the data\n",
    "print(\"First few rows of the dataframe:\")\n",
    "print(df.head())\n",
    "\n",
    "# Check for missing values in each column\n",
    "print(\"\\nMissing values in each column:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Check the data types of each column\n",
    "print(\"\\nData types of each column:\")\n",
    "print(df.dtypes)\n",
    "\n",
    "# Check for any non-numeric values in the dataset\n",
    "print(\"\\nUnique values in each column (for non-numeric checks):\")\n",
    "for column in df.select_dtypes(include=['object']).columns:\n",
    "    print(f\"{column}: {df[column].unique()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Unique values in 'Dependents' column after replacement:\n",
      "[0 1 2 3]\n"
     ]
    }
   ],
   "source": [
    "# Replace '3+' with '3' in the 'Dependents' column\n",
    "df['Dependents'] = df['Dependents'].replace('3+', '3')\n",
    "\n",
    "# Convert 'Dependents' column to numeric type\n",
    "df['Dependents'] = pd.to_numeric(df['Dependents'])\n",
    "\n",
    "# Check the unique values again to ensure everything is now numeric\n",
    "print(\"\\nUnique values in 'Dependents' column after replacement:\")\n",
    "print(df['Dependents'].unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data types after handling 'Dependents' column:\n",
      "Loan_ID                     object\n",
      "Dependents                   int64\n",
      "ApplicantIncome              int64\n",
      "CoapplicantIncome          float64\n",
      "LoanAmount                 float64\n",
      "Credit_History             float64\n",
      "Loan_Status                  int64\n",
      "Gender_Male                   bool\n",
      "Married_Yes                   bool\n",
      "Education_Not Graduate        bool\n",
      "Self_Employed_Yes             bool\n",
      "Property_Area_Semiurban       bool\n",
      "Property_Area_Urban           bool\n",
      "Monthly_Loan_Payment       float64\n",
      "dtype: object\n",
      "\n",
      "Columns after one-hot encoding:\n",
      "Index(['Dependents', 'ApplicantIncome', 'CoapplicantIncome', 'LoanAmount',\n",
      "       'Credit_History', 'Loan_Status', 'Gender_Male', 'Married_Yes',\n",
      "       'Education_Not Graduate', 'Self_Employed_Yes',\n",
      "       ...\n",
      "       'Loan_ID_LP002959', 'Loan_ID_LP002960', 'Loan_ID_LP002961',\n",
      "       'Loan_ID_LP002964', 'Loan_ID_LP002974', 'Loan_ID_LP002978',\n",
      "       'Loan_ID_LP002979', 'Loan_ID_LP002983', 'Loan_ID_LP002984',\n",
      "       'Loan_ID_LP002990'],\n",
      "      dtype='object', length=626)\n",
      "\n",
      "Shapes of train and test sets:\n",
      "X_train shape: (491, 625)\n",
      "X_test shape: (123, 625)\n"
     ]
    }
   ],
   "source": [
    "# Check for any remaining categorical variables\n",
    "print(\"\\nData types after handling 'Dependents' column:\")\n",
    "print(df.dtypes)\n",
    "\n",
    "# Convert categorical columns (binary variables) to numeric using one-hot encoding\n",
    "df = pd.get_dummies(df, drop_first=True)\n",
    "\n",
    "# Verify the transformation\n",
    "print(\"\\nColumns after one-hot encoding:\")\n",
    "print(df.columns)\n",
    "\n",
    "# Split the data into features (X) and target variable (y)\n",
    "X = df.drop(columns=['Loan_Status'])\n",
    "y = df['Loan_Status']\n",
    "\n",
    "# Split the data into training and test sets (80-20 split)\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Print the shapes of the train and test sets to confirm the split\n",
    "print(\"\\nShapes of train and test sets:\")\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Performance:\n",
      "Accuracy: 0.7886\n",
      "\n",
      "Confusion Matrix:\n",
      "[[18 25]\n",
      " [ 1 79]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.42      0.58        43\n",
      "           1       0.76      0.99      0.86        80\n",
      "\n",
      "    accuracy                           0.79       123\n",
      "   macro avg       0.85      0.70      0.72       123\n",
      "weighted avg       0.83      0.79      0.76       123\n",
      "\n",
      "\n",
      "Model Coefficients:\n",
      "[[ 7.04011691e-02 -1.07307297e-05 -4.61685448e-05 -9.97436731e-04\n",
      "   3.86246342e+00  5.91146020e-02  6.24268947e-01 -1.77647433e-01\n",
      "   1.44390452e-01  1.07615043e+00  1.71478925e-01 -8.75322169e-06\n",
      "  -5.46589039e-01  0.00000000e+00  2.03751833e-01  2.04139440e-01\n",
      "   1.99917678e-01  0.00000000e+00 -2.33758813e-01  1.34586024e-01\n",
      "  -4.69925190e-01  0.00000000e+00  0.00000000e+00  1.88334983e-01\n",
      "  -4.66846012e-01  7.31965910e-02  1.92334300e-01  2.30759073e-01\n",
      "  -1.49075089e-01 -4.72087211e-01  1.39822269e-01 -1.19625377e-01\n",
      "   2.36704013e-01 -1.66082017e-01 -1.33723664e-01  0.00000000e+00\n",
      "   9.69429036e-02  8.25601213e-02  1.59240057e-01 -4.49444997e-01\n",
      "   0.00000000e+00  0.00000000e+00 -4.95085168e-01 -5.12549717e-01\n",
      "   7.86562242e-02 -3.98219535e-01  1.39559451e-01 -1.56743142e-01\n",
      "   7.16516409e-02  2.84266056e-01  0.00000000e+00 -4.91063789e-01\n",
      "   1.50461728e-01  0.00000000e+00  8.72788521e-02  1.66633813e-01\n",
      "   1.38309409e-01  8.67787399e-02  1.40592343e-01 -1.59629602e-01\n",
      "   1.15514744e-01  8.94086549e-02  9.67505575e-02 -5.67579281e-01\n",
      "  -5.51125191e-01 -1.16776955e-01  0.00000000e+00  7.28386011e-02\n",
      "  -5.27004461e-01  1.60535608e-01  1.90268189e-01  0.00000000e+00\n",
      "   8.86178693e-02 -9.69408159e-02  0.00000000e+00 -2.02000725e-01\n",
      "  -5.00611753e-01  0.00000000e+00  2.05226563e-01  1.42695337e-01\n",
      "  -1.88451906e-01  0.00000000e+00  8.72280989e-02  0.00000000e+00\n",
      "   0.00000000e+00  7.64779168e-02 -4.69586872e-01  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  9.24738950e-02  9.39667239e-02\n",
      "   0.00000000e+00  0.00000000e+00 -5.11406017e-01  8.78627591e-02\n",
      "   1.36281152e-01  0.00000000e+00  7.66859095e-02  2.58475971e-01\n",
      "   1.10936339e-01  8.76650590e-02  1.02667536e-01  1.46123842e-01\n",
      "   1.19455026e-01  1.27499984e-01 -4.56335750e-01  7.15697417e-02\n",
      "   6.09374947e-02  1.15980815e-01  8.92219002e-02  1.86011905e-01\n",
      "   0.00000000e+00  1.37499787e-01  8.98289055e-02  0.00000000e+00\n",
      "   1.09713442e-01  1.71534802e-01 -3.98294100e-01 -1.10699640e-01\n",
      "   0.00000000e+00  0.00000000e+00  7.23859983e-02 -9.50863477e-02\n",
      "   8.73994016e-02  1.18327885e-01  1.42504517e-01  7.67702573e-02\n",
      "   1.04486769e-01  0.00000000e+00  2.56696780e-01  1.61140306e-01\n",
      "   7.48068319e-02  5.20099077e-01  5.40625176e-02  2.38542107e-01\n",
      "   1.31657249e-01  1.69731432e-01  1.89130551e-01 -1.27119278e-01\n",
      "  -4.94151998e-01  2.98904829e-01  0.00000000e+00  0.00000000e+00\n",
      "   5.17284730e-02  1.11629008e-01  0.00000000e+00 -5.82372434e-01\n",
      "   0.00000000e+00 -1.04008172e-01 -5.00212455e-01  0.00000000e+00\n",
      "   2.23046609e-01  1.38542099e-01  8.18181600e-02  1.42489046e-01\n",
      "   0.00000000e+00  7.51667371e-02  8.50928525e-02  0.00000000e+00\n",
      "   8.00033804e-02 -1.17988973e-01  1.39375488e-01 -3.62234241e-01\n",
      "   0.00000000e+00  1.46998911e-01  0.00000000e+00  1.23858502e-01\n",
      "   1.53965345e-01  0.00000000e+00  1.59844329e-01  1.11524836e-01\n",
      "  -4.34003057e-01 -2.35154164e-01  0.00000000e+00  1.10054006e-01\n",
      "   1.63631227e-01 -5.71877537e-01  1.33555732e-01 -2.06468315e-01\n",
      "   8.79028075e-02  1.66985229e-01  2.05263587e-01 -5.48127595e-01\n",
      "   1.22739095e-01  0.00000000e+00  1.21663828e-01  0.00000000e+00\n",
      "  -1.27130094e-01  6.71339975e-02 -8.48766559e-02 -1.02033682e-01\n",
      "   0.00000000e+00  0.00000000e+00 -5.18702150e-01  5.90454331e-02\n",
      "   1.04492043e-01 -2.20130085e-01  9.80885085e-02  0.00000000e+00\n",
      "   1.54446745e-01  1.98727995e-01 -5.10723447e-01  0.00000000e+00\n",
      "   1.01316417e-01  1.74784434e-01 -5.72915345e-01  0.00000000e+00\n",
      "   2.34914935e-01  6.69804495e-02 -4.93484677e-01  1.03059199e-01\n",
      "   4.95697787e-01 -5.08166125e-01  1.89889347e-01  0.00000000e+00\n",
      "   1.41090653e-01  1.34241638e-01  2.18223521e-01  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00 -2.62530769e-01  0.00000000e+00\n",
      "   1.30838479e-01  1.49924575e-01  8.41471080e-02 -5.31444428e-01\n",
      "   8.43348876e-02  0.00000000e+00  5.86936939e-02 -1.86695294e-01\n",
      "   6.56878136e-02  1.42020483e-01  9.95861659e-02  7.90047156e-02\n",
      "  -5.18558193e-01 -4.63020484e-01  9.70680764e-02  0.00000000e+00\n",
      "   2.19935381e-01  6.91105063e-02  0.00000000e+00  2.65009340e-01\n",
      "   1.69993336e-01  0.00000000e+00  0.00000000e+00 -4.41079359e-01\n",
      "   1.47807881e-01  0.00000000e+00  6.23027939e-02  1.15953416e-01\n",
      "  -4.57596054e-01  2.49311842e-01  9.09069359e-02  1.39197189e-01\n",
      "  -4.46236908e-01  1.04388510e-01  1.87480492e-01  0.00000000e+00\n",
      "   1.19567422e-01  0.00000000e+00 -5.16588677e-01  2.36762502e-01\n",
      "   2.27536752e-01 -8.22806111e-02  1.89025955e-01 -8.96241773e-02\n",
      "   0.00000000e+00 -4.31340815e-01 -5.74105821e-01  0.00000000e+00\n",
      "   1.33555721e-01 -5.60347368e-01  1.47429974e-01  1.15104760e-01\n",
      "   2.16275493e-01  7.41684833e-02  5.05510265e-01 -5.02830136e-01\n",
      "   1.34364293e-01  1.12415608e-01  1.39974851e-01  1.88254079e-01\n",
      "   9.18025345e-02  5.60032944e-02  0.00000000e+00  1.21078750e-01\n",
      "   0.00000000e+00  1.41794573e-01  1.41669656e-01 -7.38223860e-02\n",
      "   0.00000000e+00  9.14259372e-02  1.28656128e-01  0.00000000e+00\n",
      "   1.72653450e-01 -5.73485388e-01  0.00000000e+00  9.54947101e-02\n",
      "   2.24916051e-01  1.34786252e-01 -2.53693554e-01  1.17005375e-01\n",
      "  -1.13932945e-01  1.04395136e-01  9.75706861e-02  0.00000000e+00\n",
      "   1.00694418e-01 -4.76909871e-01 -5.37718644e-01  0.00000000e+00\n",
      "   0.00000000e+00  1.52454013e-01  1.10312896e-01  2.02491421e-01\n",
      "  -3.80320669e-01  1.61914710e-01 -1.26485223e-01 -3.54236268e-01\n",
      "   1.87720386e-01  8.48870289e-02  1.83605235e-01  0.00000000e+00\n",
      "   9.97842916e-02 -5.15711315e-01  1.77452871e-01  0.00000000e+00\n",
      "   1.19964743e-01  8.59468574e-02  0.00000000e+00  7.99647603e-02\n",
      "   5.91408464e-02  0.00000000e+00  1.66771277e-01  1.64176313e-01\n",
      "  -9.14115788e-02  5.55538597e-01  0.00000000e+00 -5.53728734e-01\n",
      "   1.27625794e-01  1.80985403e-01  1.08841086e-01  1.57766871e-01\n",
      "   0.00000000e+00  1.51656529e-01  7.11815454e-02  1.26512516e-01\n",
      "   1.32344723e-01 -1.12253623e-01  8.17307482e-02  0.00000000e+00\n",
      "  -5.48832460e-01  2.07482689e-01  7.32833068e-02  1.05682593e-01\n",
      "   9.63171108e-02 -9.77456723e-02  1.74298953e-01  1.58044263e-01\n",
      "   1.83493917e-01  0.00000000e+00 -3.35334113e-01  8.63077322e-02\n",
      "  -2.10746827e-01  6.49463051e-02  0.00000000e+00  1.31126649e-01\n",
      "  -5.70568532e-01 -1.16970973e-01  9.06320225e-02 -5.87122747e-01\n",
      "   0.00000000e+00  1.43679417e-01  0.00000000e+00  1.79641197e-01\n",
      "  -3.89301471e-01 -5.51991992e-01 -1.15665801e-01  0.00000000e+00\n",
      "  -4.04468464e-01  7.77372261e-02  7.18517867e-02  2.08854778e-01\n",
      "  -1.13199687e-01  1.56907064e-01  1.31682836e-01  1.13010788e-01\n",
      "   0.00000000e+00 -4.96962774e-01  1.09919600e-01  8.55330484e-02\n",
      "   2.28429747e-01  0.00000000e+00  1.85394714e-01 -5.66858217e-01\n",
      "   1.67720729e-01  1.91489859e-01 -1.01020549e-01  1.45702886e-01\n",
      "   1.18525683e-01  1.76905196e-01  1.60864857e-01  1.35777580e-01\n",
      "   9.81822079e-02  0.00000000e+00  6.69925878e-02  0.00000000e+00\n",
      "   1.20859406e-01  0.00000000e+00 -2.02748678e-01 -1.64488666e-01\n",
      "  -4.43492915e-01  2.14469473e-01  1.09206297e-01 -4.72131944e-01\n",
      "   9.57064603e-02  2.00278064e-01  1.68804535e-01 -2.17236216e-01\n",
      "  -9.08091799e-02 -5.65380870e-01  1.49910479e-01  0.00000000e+00\n",
      "   1.77937622e-01 -1.46230597e-01  0.00000000e+00 -4.90494459e-01\n",
      "  -3.69386225e-01  1.21791128e-01  8.52203232e-02  1.24898786e-01\n",
      "   0.00000000e+00  1.23543400e-01 -1.57319744e-01  7.40660988e-02\n",
      "   0.00000000e+00 -4.89857608e-01  6.61538884e-02  1.01147158e-01\n",
      "   2.20334690e-01  9.56452257e-02  0.00000000e+00  2.07333448e-01\n",
      "   8.96125171e-02  1.75358622e-01  6.58733985e-02  9.82416185e-02\n",
      "   0.00000000e+00 -8.54607825e-02  0.00000000e+00  1.79521438e-01\n",
      "   1.58306726e-01  1.60148695e-01  1.44265559e-01  1.91634910e-01\n",
      "   1.06458246e-01  1.39532924e-01 -5.68012522e-01 -1.61530086e-01\n",
      "   0.00000000e+00  0.00000000e+00  1.32626345e-01  0.00000000e+00\n",
      "   4.92577877e-01  9.93691597e-02  5.71488758e-02  1.26122204e-01\n",
      "   0.00000000e+00  1.56300514e-01 -5.73639744e-01  9.58091009e-02\n",
      "   1.01680984e-01  0.00000000e+00  1.34790238e-01 -1.83573716e-01\n",
      "   1.88935609e-01 -1.53886077e-01  7.87132827e-02  7.61797594e-02\n",
      "   0.00000000e+00  8.14876373e-02 -1.26735626e-01  5.39395906e-02\n",
      "   0.00000000e+00  2.00055637e-01  1.11580022e-01  1.06228173e-01\n",
      "  -2.39830930e-01  0.00000000e+00 -5.47774694e-01  2.30228067e-01\n",
      "   1.28598715e-01  8.70580060e-02  1.24355807e-01  1.07517468e-01\n",
      "   0.00000000e+00 -1.59148751e-01 -4.43096805e-01  7.43659682e-02\n",
      "  -4.95121295e-01  1.76579864e-01  2.27978120e-01  2.40054356e-01\n",
      "   2.06982124e-01 -1.53077466e-01  5.50783495e-02  1.74488878e-01\n",
      "   1.36054773e-01  5.80794406e-02 -7.20645274e-02  1.57823383e-01\n",
      "   8.99612788e-02  1.10701867e-01 -4.53483790e-01  1.26284226e-01\n",
      "   0.00000000e+00  2.75541139e-01 -4.81004683e-01  1.71109442e-01\n",
      "   0.00000000e+00 -4.08424879e-01  1.26976341e-01  1.25993527e-01\n",
      "  -5.84729243e-01 -3.41942426e-01  1.13086305e-01  5.35640098e-02\n",
      "  -1.62624294e-01 -4.93234706e-01  0.00000000e+00  7.69462627e-02\n",
      "   9.36211103e-02  8.18225716e-02  2.03516352e-01 -5.25191008e-01\n",
      "   1.69246373e-01  7.27654162e-02  5.09508559e-01  1.51804944e-01\n",
      "   1.84802037e-01  1.97731487e-01  8.94725041e-02 -1.42832936e-01\n",
      "  -5.07478006e-01  2.26205021e-01  2.65104123e-01  2.43570434e-01\n",
      "   9.36661658e-02 -5.12602911e-01  1.04064051e-01  6.81085931e-02\n",
      "  -2.15192971e-01  8.08426434e-02  0.00000000e+00  8.73431219e-02\n",
      "   1.40858593e-01 -5.02411353e-01  2.00152033e-01  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  1.75103241e-01  1.49259267e-01\n",
      "  -8.54725848e-02 -1.36689276e-01  5.27854264e-02  1.43583413e-01\n",
      "   1.00527980e-01  0.00000000e+00  8.66029407e-02  9.22761787e-02\n",
      "   1.03049876e-01  1.77767028e-01  1.74541490e-01 -1.04472730e-01\n",
      "   1.74516007e-01  1.57612534e-01 -1.87460153e-01  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00 -1.21329662e-01  1.93857011e-01\n",
      "  -5.25633903e-01  0.00000000e+00  0.00000000e+00 -2.04719087e-01\n",
      "   0.00000000e+00  0.00000000e+00  2.35126971e-01  0.00000000e+00\n",
      "  -2.50923155e-01  4.68685046e-02 -5.82246885e-01  0.00000000e+00\n",
      "  -5.14807635e-01  1.24875570e-01  1.16134460e-01  1.08423159e-01\n",
      "   0.00000000e+00  6.85163943e-02 -5.82581717e-01  1.52756816e-01\n",
      "   1.41194173e-01  2.14506143e-01  0.00000000e+00 -4.72455131e-01\n",
      "  -1.90397611e-01  1.41361468e-01  1.41756942e-01 -1.81705737e-01\n",
      "   2.13293116e-01  0.00000000e+00  2.36993062e-01  1.05960545e-01\n",
      "  -4.36115755e-01  9.23850182e-02  1.74296345e-01  1.17782980e-01\n",
      "   0.00000000e+00  0.00000000e+00  1.98393746e-01  1.39975707e-01\n",
      "  -1.99390539e-01]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\MoaviaHassan\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "# Initialize the logistic regression model\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "\n",
    "# Train the model on the training data\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model performance\n",
    "print(\"\\nModel Performance:\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# If needed, you can also print the coefficients of the logistic regression model\n",
    "print(\"\\nModel Coefficients:\")\n",
    "print(model.coef_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Performance Analysis\n",
    "\n",
    "### Key Observations:\n",
    "\n",
    "1. **Confusion Matrix**:\n",
    "   - **True Positives (79)**: The model correctly predicted **79** loan approvals (class `1`).\n",
    "   - **True Negatives (18)**: The model correctly predicted **18** loan denials (class `0`), but this is quite low.\n",
    "   - **False Positives (25)**: The model incorrectly predicted **25** loan denials as approvals.\n",
    "   - **False Negatives (1)**: The model incorrectly predicted only **1** loan approval as a denial, which is very few.\n",
    "\n",
    "2. **Classification Report**:\n",
    "   - **Recall for Class 1 (Loan Approval)**: **99%**, indicating that the model is very good at identifying loan approvals.\n",
    "   - **Recall for Class 0 (Loan Denial)**: **42%**, indicating that the model misses many loan denials.\n",
    "   - **Precision for Class 0**: **95%**, suggesting that when the model predicts a loan denial, it is often correct. However, this is misleading due to the low number of true positives.\n",
    "\n",
    "3. **Model Coefficients**:\n",
    "   - The coefficients give insights into how each feature affects the model's predictions. For example, certain features like `Gender_Male` and `Monthly_Loan_Payment` have a significant impact on the prediction.\n",
    "\n",
    "4. **Convergence Warning**:\n",
    "   - A **ConvergenceWarning** was raised, indicating that the solver (lbfgs) did not converge within the specified number of iterations. This might be due to unscaled data or the complexity of the problem. It is recommended to either increase the `max_iter` parameter or scale the data for better convergence.\n",
    "\n",
    "---\n",
    "\n",
    "### Recommendations for Improvement:\n",
    "\n",
    "1. **Scaling the Data**: Standardizing the features using `StandardScaler` could improve the model's performance and help the solver converge more efficiently.\n",
    "\n",
    "2. **Hyperparameter Tuning**: Tuning the hyperparameters (e.g., `C` for regularization) or using a different solver like `saga` may help improve model convergence and performance.\n",
    "\n",
    "3. **Addressing Class Imbalance**: \n",
    "   - The model is biased towards predicting loan approvals (class `1`). To address this, consider using techniques like **oversampling** the minority class (class `0`) or adjusting **class weights** in the logistic regression model to improve predictions for loan denials.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Scaling the Data**\n",
    "\n",
    "Scaling the data is a crucial step when using models like logistic regression. It helps to improve convergence and model accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Model Hyperparameter Tuning**\n",
    "We can improve the model by fine-tuning its hyperparameters, like the regularization strength (C) or using a different solver."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Best Parameters: {'C': 1, 'solver': 'saga'}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define parameter grid\n",
    "param_grid = {\n",
    "    'C': [0.001, 0.01, 0.1, 1, 10],\n",
    "    'solver': ['lbfgs', 'saga']\n",
    "}\n",
    "\n",
    "# Initialize LogisticRegression\n",
    "log_reg = LogisticRegression(max_iter=1000)\n",
    "\n",
    "# Initialize GridSearchCV\n",
    "grid_search = GridSearchCV(estimator=log_reg, param_grid=param_grid, cv=5, verbose=1)\n",
    "\n",
    "# Fit the model with hyperparameter tuning\n",
    "grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Get the best parameters\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Model Evaluation After Scaling and Tuning**\n",
    "Once scaling and hyperparameter tuning are done, we evaluate the model on the test data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy after scaling and tuning: 0.7724\n",
      "\n",
      "Confusion Matrix:\n",
      "[[16 27]\n",
      " [ 1 79]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.37      0.53        43\n",
      "           1       0.75      0.99      0.85        80\n",
      "\n",
      "    accuracy                           0.77       123\n",
      "   macro avg       0.84      0.68      0.69       123\n",
      "weighted avg       0.81      0.77      0.74       123\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Use the best estimator from grid search\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred_scaled = best_model.predict(X_test_scaled)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy_scaled = accuracy_score(y_test, y_pred_scaled)\n",
    "print(f\"Accuracy after scaling and tuning: {accuracy_scaled:.4f}\")\n",
    "\n",
    "# Confusion matrix and classification report\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred_scaled))\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_scaled))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
